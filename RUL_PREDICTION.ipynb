{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4TINIYq46sMstJt2rbIJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salehin555/MOE-RUL-Prediction/blob/main/RUL_PREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD001"
      ],
      "metadata": {
        "id": "j8dbLAMpsojL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD001\"]\n",
        "TARGETS = [\"FD001\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqWRossENQMO",
        "outputId": "9bff4e2f-dc21-440d-8499-f34ba515b920"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=0.1561 | R2=0.5270 RMSE=28.475\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.1708 | R2=0.6182 RMSE=25.583\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.3969 | R2=0.6275 RMSE=25.270\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.7160 | R2=0.7017 RMSE=22.614\n",
            "[Epoch 05] s=0.00 topk=0 val=-0.8247 | R2=0.5785 RMSE=26.882\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.7446 | R2=0.4484 RMSE=30.750\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.2552 | R2=0.7263 RMSE=21.662\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.3979 | R2=0.7389 RMSE=21.156\n",
            "[Epoch 09] s=0.00 topk=0 val=1.6415 | R2=-0.0068 RMSE=41.544\n",
            "[Epoch 10] s=0.00 topk=0 val=-1.1568 | R2=0.6992 RMSE=22.709\n",
            "[Epoch 11] s=0.00 topk=0 val=-1.0664 | R2=0.5933 RMSE=26.403\n",
            "[Epoch 12] s=0.00 topk=0 val=-1.1788 | R2=0.7430 RMSE=20.990\n",
            "[Epoch 13] s=0.05 topk=1 val=-1.4030 | R2=0.7206 RMSE=21.886\n",
            "[Epoch 14] s=0.10 topk=1 val=-0.3117 | R2=-0.1598 RMSE=44.588\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.3494 | R2=0.7115 RMSE=22.237\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.3317 | R2=0.6758 RMSE=23.575\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.0910 | R2=0.6094 RMSE=25.875\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.3731 | R2=0.7439 RMSE=20.952\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.4185 | R2=0.7631 RMSE=20.154\n",
            "[Epoch 20] s=0.40 topk=1 val=-1.3033 | R2=0.6481 RMSE=24.560\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.4330 | R2=0.7896 RMSE=18.990\n",
            "[Epoch 22] s=0.50 topk=1 val=-1.1973 | R2=0.6925 RMSE=22.960\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.4546 | R2=0.7709 RMSE=19.817\n",
            "[Epoch 24] s=0.60 topk=1 val=-1.5270 | R2=0.7869 RMSE=19.115\n",
            "[Epoch 25] s=0.65 topk=1 val=-1.3113 | R2=0.6797 RMSE=23.433\n",
            "[Epoch 26] s=0.70 topk=1 val=-1.2081 | R2=0.7059 RMSE=22.454\n",
            "[Epoch 27] s=0.75 topk=1 val=-1.3360 | R2=0.6925 RMSE=22.959\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.4625 | R2=0.7594 RMSE=20.309\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.4295 | R2=0.7512 RMSE=20.652\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.3847 | R2=0.7538 RMSE=20.545\n",
            "[Epoch 31] s=0.95 topk=1 val=-1.5255 | R2=0.7022 RMSE=22.593\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.5791 | R2=0.7304 RMSE=21.497\n",
            "[Epoch 33] s=1.00 topk=1 val=-1.5779 | R2=0.7823 RMSE=19.319\n",
            "[Epoch 34] s=1.00 topk=1 val=-1.4432 | R2=0.6956 RMSE=22.842\n",
            "[Epoch 35] s=1.00 topk=1 val=-1.0393 | R2=0.6836 RMSE=23.288\n",
            "[Epoch 36] s=1.00 topk=1 val=-1.4538 | R2=0.7446 RMSE=20.925\n",
            "[Epoch 37] s=1.00 topk=1 val=-1.6428 | R2=0.7645 RMSE=20.092\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.5717 | R2=0.7448 RMSE=20.918\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.5950 | R2=0.7333 RMSE=21.382\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.2931 | R2=0.7735 RMSE=19.704\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.3739 | R2=0.6719 RMSE=23.715\n",
            "[Epoch 42] s=1.00 topk=1 val=-1.4606 | R2=0.7297 RMSE=21.527\n",
            "[Epoch 43] s=1.00 topk=1 val=-1.3636 | R2=0.6139 RMSE=25.728\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.3144 | R2=0.7896 RMSE=18.993\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.5231 | R2=0.7601 RMSE=20.278\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.1633 | R2=0.6857 RMSE=23.210\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.5693 | R2=0.7410 RMSE=21.072\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.5969 | R2=0.7836 RMSE=19.262\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.3612 | R2=0.7961 RMSE=18.695\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.5803 | R2=0.7831 RMSE=19.284\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.6526 | R2=0.7791 RMSE=19.461\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.6530 | R2=0.7518 RMSE=20.629\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.6800 | R2=0.7599 RMSE=20.286\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.6087 | R2=0.6771 RMSE=23.529\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.4824 | R2=0.6874 RMSE=23.150\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.6999 | R2=0.7374 RMSE=21.218\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.6454 | R2=0.7531 RMSE=20.571\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.5887 | R2=0.7519 RMSE=20.625\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.6410 | R2=0.7619 RMSE=20.205\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.7122 | R2=0.7735 RMSE=19.706\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.6955 | R2=0.7517 RMSE=20.631\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.7319 | R2=0.7616 RMSE=20.217\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.6455 | R2=0.7364 RMSE=21.258\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.7263 | R2=0.7862 RMSE=19.145\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.6922 | R2=0.7959 RMSE=18.707\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.7307 | R2=0.7975 RMSE=18.630\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.7200 | R2=0.7865 RMSE=19.129\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.7467 | R2=0.7908 RMSE=18.935\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.7434 | R2=0.7883 RMSE=19.051\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.7289 | R2=0.7944 RMSE=18.773\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.7354 | R2=0.8004 RMSE=18.496\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.7410 | R2=0.7851 RMSE=19.192\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.6696 | R2=0.7781 RMSE=19.502\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.6934 | R2=0.7831 RMSE=19.283\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.7697 | R2=0.7903 RMSE=18.960\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.7611 | R2=0.7928 RMSE=18.845\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.7898 | R2=0.7945 RMSE=18.771\n",
            "[Epoch 78] s=1.00 topk=1 val=-1.7860 | R2=0.7921 RMSE=18.879\n",
            "[Epoch 79] s=1.00 topk=1 val=-1.7774 | R2=0.7917 RMSE=18.897\n",
            "[Epoch 80] s=1.00 topk=1 val=-1.7857 | R2=0.7896 RMSE=18.989\n",
            "[Epoch 81] s=1.00 topk=1 val=-1.7756 | R2=0.7859 RMSE=19.159\n",
            "[Epoch 82] s=1.00 topk=1 val=-1.7782 | R2=0.7882 RMSE=19.054\n",
            "[Epoch 83] s=1.00 topk=1 val=-1.7821 | R2=0.7835 RMSE=19.265\n",
            "[Epoch 84] s=1.00 topk=1 val=-1.7835 | R2=0.7906 RMSE=18.948\n",
            "[Epoch 85] s=1.00 topk=1 val=-1.7844 | R2=0.7894 RMSE=19.001\n",
            "\n",
            "==============================\n",
            " Target FD001 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.8602 RMSE=14.984 MAE=11.038\n",
            "UQ: 90% PI coverage=0.900 | mean width=49.980 | cal_scale=0.800\n",
            "Gate usage (avg weights): [0.4411 0.134  0.223  0.202 ]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oKpyAxUtNQJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rVRCmUrNQHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD002"
      ],
      "metadata": {
        "id": "w18ZsiaThpbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD002\"]\n",
        "TARGETS = [\"FD002\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO4SpMn7XzQ4",
        "outputId": "50c8bb40-62fb-47f2-c52b-e09997912eec"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=-0.2407 | R2=-0.0040 RMSE=41.841\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.5685 | R2=0.0405 RMSE=40.904\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.8230 | R2=0.5002 RMSE=29.521\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.8968 | R2=0.5992 RMSE=26.435\n",
            "[Epoch 05] s=0.00 topk=0 val=-0.8805 | R2=0.5406 RMSE=28.303\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.9473 | R2=0.5696 RMSE=27.395\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.1210 | R2=0.6504 RMSE=24.692\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.1902 | R2=0.7201 RMSE=22.093\n",
            "[Epoch 09] s=0.00 topk=0 val=-0.9997 | R2=0.4966 RMSE=29.627\n",
            "[Epoch 10] s=0.00 topk=0 val=-1.1631 | R2=0.6726 RMSE=23.893\n",
            "[Epoch 11] s=0.00 topk=0 val=-0.8876 | R2=0.4254 RMSE=31.653\n",
            "[Epoch 12] s=0.00 topk=0 val=-0.9271 | R2=0.4042 RMSE=32.233\n",
            "[Epoch 13] s=0.05 topk=1 val=-0.8902 | R2=0.6009 RMSE=26.379\n",
            "[Epoch 14] s=0.10 topk=1 val=-0.5136 | R2=0.7117 RMSE=22.419\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.1794 | R2=0.6934 RMSE=23.123\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.1896 | R2=0.7095 RMSE=22.508\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.1095 | R2=0.6102 RMSE=26.069\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.0835 | R2=0.6793 RMSE=23.647\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.1647 | R2=0.6793 RMSE=23.648\n",
            "[Epoch 20] s=0.40 topk=1 val=-0.7054 | R2=0.2853 RMSE=35.302\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.2020 | R2=0.7009 RMSE=22.837\n",
            "[Epoch 22] s=0.50 topk=1 val=-1.2153 | R2=0.7169 RMSE=22.220\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.0153 | R2=0.5870 RMSE=26.836\n",
            "[Epoch 24] s=0.60 topk=1 val=-1.2576 | R2=0.7014 RMSE=22.818\n",
            "[Epoch 25] s=0.65 topk=1 val=-1.3112 | R2=0.7348 RMSE=21.503\n",
            "[Epoch 26] s=0.70 topk=1 val=-0.9807 | R2=0.7528 RMSE=20.760\n",
            "[Epoch 27] s=0.75 topk=1 val=-0.9184 | R2=0.5170 RMSE=29.022\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.2618 | R2=0.7269 RMSE=21.823\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.0039 | R2=0.6235 RMSE=25.623\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.3316 | R2=0.7510 RMSE=20.838\n",
            "[Epoch 31] s=0.95 topk=1 val=-1.0646 | R2=0.5661 RMSE=27.505\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.2939 | R2=0.7387 RMSE=21.346\n",
            "[Epoch 33] s=1.00 topk=1 val=-1.2091 | R2=0.6180 RMSE=25.807\n",
            "[Epoch 34] s=1.00 topk=1 val=-1.2384 | R2=0.7222 RMSE=22.011\n",
            "[Epoch 35] s=1.00 topk=1 val=-1.3076 | R2=0.7428 RMSE=21.178\n",
            "[Epoch 36] s=1.00 topk=1 val=-1.2329 | R2=0.7287 RMSE=21.751\n",
            "[Epoch 37] s=1.00 topk=1 val=-0.7625 | R2=0.5482 RMSE=28.068\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.3025 | R2=0.7408 RMSE=21.260\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.0330 | R2=0.6239 RMSE=25.610\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.3740 | R2=0.7619 RMSE=20.375\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.3331 | R2=0.7556 RMSE=20.643\n",
            "[Epoch 42] s=1.00 topk=1 val=-0.9993 | R2=0.7386 RMSE=21.348\n",
            "[Epoch 43] s=1.00 topk=1 val=-1.3101 | R2=0.7572 RMSE=20.578\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.3435 | R2=0.7487 RMSE=20.933\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.2292 | R2=0.7172 RMSE=22.206\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.2736 | R2=0.7038 RMSE=22.727\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.2271 | R2=0.6859 RMSE=23.404\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.3371 | R2=0.7704 RMSE=20.009\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.1921 | R2=0.6959 RMSE=23.029\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.3824 | R2=0.7483 RMSE=20.952\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.2435 | R2=0.6419 RMSE=24.987\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.3536 | R2=0.7647 RMSE=20.257\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.3232 | R2=0.7417 RMSE=21.224\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.3653 | R2=0.7579 RMSE=20.547\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.2940 | R2=0.7140 RMSE=22.331\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.2491 | R2=0.7385 RMSE=21.352\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.3916 | R2=0.7645 RMSE=20.265\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.3706 | R2=0.7695 RMSE=20.046\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.3701 | R2=0.7552 RMSE=20.661\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.2851 | R2=0.7315 RMSE=21.638\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.3485 | R2=0.7504 RMSE=20.861\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.0526 | R2=0.7327 RMSE=21.589\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.3591 | R2=0.7569 RMSE=20.589\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.3811 | R2=0.7670 RMSE=20.155\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.3697 | R2=0.7767 RMSE=19.733\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.4193 | R2=0.7618 RMSE=20.379\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.4428 | R2=0.7772 RMSE=19.711\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.4294 | R2=0.7671 RMSE=20.153\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.3394 | R2=0.7391 RMSE=21.329\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.4267 | R2=0.7709 RMSE=19.987\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.4237 | R2=0.7731 RMSE=19.890\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.4297 | R2=0.7796 RMSE=19.604\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.4309 | R2=0.7774 RMSE=19.702\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.4117 | R2=0.7824 RMSE=19.477\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.4335 | R2=0.7763 RMSE=19.749\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.4179 | R2=0.7693 RMSE=20.055\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.4310 | R2=0.7731 RMSE=19.889\n",
            "[Epoch 78] s=1.00 topk=1 val=-1.4396 | R2=0.7810 RMSE=19.540\n",
            "[Epoch 79] s=1.00 topk=1 val=-1.4503 | R2=0.7823 RMSE=19.483\n",
            "[Epoch 80] s=1.00 topk=1 val=-1.4410 | R2=0.7856 RMSE=19.334\n",
            "[Epoch 81] s=1.00 topk=1 val=-1.4413 | R2=0.7819 RMSE=19.503\n",
            "[Epoch 82] s=1.00 topk=1 val=-1.4522 | R2=0.7831 RMSE=19.446\n",
            "[Epoch 83] s=1.00 topk=1 val=-1.4476 | R2=0.7816 RMSE=19.514\n",
            "[Epoch 84] s=1.00 topk=1 val=-1.4533 | R2=0.7823 RMSE=19.481\n",
            "[Epoch 85] s=1.00 topk=1 val=-1.4359 | R2=0.7831 RMSE=19.445\n",
            "\n",
            "==============================\n",
            " Target FD002 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.8102 RMSE=18.628 MAE=14.563\n",
            "UQ: 90% PI coverage=0.874 | mean width=50.521 | cal_scale=0.720\n",
            "Gate usage (avg weights): [0.1726 0.1753 0.3413 0.3108]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD003"
      ],
      "metadata": {
        "id": "FRjf7sTPsSEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD003\"]\n",
        "TARGETS = [\"FD003\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H-jVN79XzJG",
        "outputId": "e26ac17f-e694-4ea0-ba6e-70a3c2dbb69c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=0.0894 | R2=0.6722 RMSE=23.885\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.3108 | R2=0.5821 RMSE=26.968\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.6049 | R2=0.6585 RMSE=24.381\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.9182 | R2=0.6446 RMSE=24.870\n",
            "[Epoch 05] s=0.00 topk=0 val=-1.3225 | R2=0.6947 RMSE=23.051\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.8848 | R2=0.6259 RMSE=25.517\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.2841 | R2=0.7014 RMSE=22.798\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.0305 | R2=0.7097 RMSE=22.478\n",
            "[Epoch 09] s=0.00 topk=0 val=-1.4801 | R2=0.7613 RMSE=20.381\n",
            "[Epoch 10] s=0.00 topk=0 val=-1.5606 | R2=0.7627 RMSE=20.325\n",
            "[Epoch 11] s=0.00 topk=0 val=-1.3519 | R2=0.7336 RMSE=21.531\n",
            "[Epoch 12] s=0.00 topk=0 val=-1.1461 | R2=0.6607 RMSE=24.303\n",
            "[Epoch 13] s=0.05 topk=1 val=-1.5128 | R2=0.8100 RMSE=18.187\n",
            "[Epoch 14] s=0.10 topk=1 val=-1.5152 | R2=0.7922 RMSE=19.018\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.0954 | R2=0.6798 RMSE=23.606\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.1003 | R2=0.6635 RMSE=24.200\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.4028 | R2=0.7300 RMSE=21.677\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.3958 | R2=0.7589 RMSE=20.485\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.6033 | R2=0.8125 RMSE=18.066\n",
            "[Epoch 20] s=0.40 topk=1 val=-1.3196 | R2=0.7734 RMSE=19.858\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.3083 | R2=0.7524 RMSE=20.760\n",
            "[Epoch 22] s=0.50 topk=1 val=-1.2262 | R2=0.7267 RMSE=21.812\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.1892 | R2=0.6974 RMSE=22.949\n",
            "[Epoch 24] s=0.60 topk=1 val=-1.3303 | R2=0.7883 RMSE=19.197\n",
            "[Epoch 25] s=0.65 topk=1 val=-1.5635 | R2=0.8291 RMSE=17.245\n",
            "[Epoch 26] s=0.70 topk=1 val=-1.6173 | R2=0.8263 RMSE=17.390\n",
            "[Epoch 27] s=0.75 topk=1 val=-1.3369 | R2=0.7830 RMSE=19.433\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.4471 | R2=0.7734 RMSE=19.859\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.4173 | R2=0.7795 RMSE=19.592\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.6158 | R2=0.8034 RMSE=18.500\n",
            "[Epoch 31] s=0.95 topk=1 val=-1.3012 | R2=0.7349 RMSE=21.480\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.5673 | R2=0.8217 RMSE=17.616\n",
            "[Epoch 33] s=1.00 topk=1 val=-1.4981 | R2=0.8258 RMSE=17.415\n",
            "[Epoch 34] s=1.00 topk=1 val=-1.5747 | R2=0.8374 RMSE=16.822\n",
            "[Epoch 35] s=1.00 topk=1 val=-1.3560 | R2=0.7902 RMSE=19.107\n",
            "[Epoch 36] s=1.00 topk=1 val=-1.5591 | R2=0.8169 RMSE=17.851\n",
            "[Epoch 37] s=1.00 topk=1 val=-1.6562 | R2=0.8566 RMSE=15.798\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.4798 | R2=0.8219 RMSE=17.606\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.5767 | R2=0.8403 RMSE=16.674\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.5006 | R2=0.8381 RMSE=16.789\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.7389 | R2=0.8431 RMSE=16.523\n",
            "[Epoch 42] s=1.00 topk=1 val=-1.4606 | R2=0.7799 RMSE=19.574\n",
            "[Epoch 43] s=1.00 topk=1 val=-1.5675 | R2=0.8301 RMSE=17.197\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.6000 | R2=0.8304 RMSE=17.181\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.6743 | R2=0.8323 RMSE=17.084\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.6420 | R2=0.8452 RMSE=16.413\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.5966 | R2=0.8458 RMSE=16.382\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.6119 | R2=0.8379 RMSE=16.799\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.5619 | R2=0.8392 RMSE=16.729\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.5228 | R2=0.8283 RMSE=17.288\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.7279 | R2=0.8647 RMSE=15.347\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.6573 | R2=0.8438 RMSE=16.491\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.7599 | R2=0.8492 RMSE=16.201\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.7744 | R2=0.8504 RMSE=16.135\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.6478 | R2=0.8596 RMSE=15.630\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.7448 | R2=0.8581 RMSE=15.715\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.7589 | R2=0.8573 RMSE=15.758\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.7816 | R2=0.8639 RMSE=15.393\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.7314 | R2=0.8517 RMSE=16.067\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.4106 | R2=0.7940 RMSE=18.933\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.7136 | R2=0.8543 RMSE=15.925\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.7533 | R2=0.8479 RMSE=16.268\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.5139 | R2=0.8401 RMSE=16.681\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.5633 | R2=0.8314 RMSE=17.128\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.7390 | R2=0.8499 RMSE=16.163\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.7458 | R2=0.8537 RMSE=15.958\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.6545 | R2=0.8509 RMSE=16.111\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.7131 | R2=0.8534 RMSE=15.975\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.7869 | R2=0.8566 RMSE=15.798\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.8149 | R2=0.8599 RMSE=15.616\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.7777 | R2=0.8610 RMSE=15.556\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.5871 | R2=0.8592 RMSE=15.656\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.4918 | R2=0.8614 RMSE=15.533\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.4946 | R2=0.8597 RMSE=15.624\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.5769 | R2=0.8595 RMSE=15.638\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.6496 | R2=0.8605 RMSE=15.582\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.7048 | R2=0.8597 RMSE=15.625\n",
            "[Epoch 78] s=1.00 topk=1 val=-1.7199 | R2=0.8626 RMSE=15.467\n",
            "[Epoch 79] s=1.00 topk=1 val=-1.7372 | R2=0.8655 RMSE=15.301\n",
            "[Epoch 80] s=1.00 topk=1 val=-1.7432 | R2=0.8619 RMSE=15.505\n",
            "[Epoch 81] s=1.00 topk=1 val=-1.7909 | R2=0.8626 RMSE=15.466\n",
            "[Epoch 82] s=1.00 topk=1 val=-1.7916 | R2=0.8658 RMSE=15.283\n",
            "[Epoch 83] s=1.00 topk=1 val=-1.7761 | R2=0.8613 RMSE=15.536\n",
            "[Epoch 84] s=1.00 topk=1 val=-1.7710 | R2=0.8661 RMSE=15.266\n",
            "Early stopping (best val=-1.8149)\n",
            "\n",
            "==============================\n",
            " Target FD003 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.8035 RMSE=17.364 MAE=13.016\n",
            "UQ: 90% PI coverage=0.860 | mean width=41.859 | cal_scale=0.700\n",
            "Gate usage (avg weights): [0.3608 0.3335 0.1128 0.1929]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD004"
      ],
      "metadata": {
        "id": "JZ9O7acDsUcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD004\"]\n",
        "TARGETS = [\"FD004\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTyhYaTuktyH",
        "outputId": "bee81858-ff8c-407a-e7fb-c1176fc639f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=-0.1370 | R2=-0.0879 RMSE=43.112\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.4181 | R2=-0.0584 RMSE=42.523\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.6923 | R2=0.5067 RMSE=29.030\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.7007 | R2=0.6014 RMSE=26.095\n",
            "[Epoch 05] s=0.00 topk=0 val=-0.7130 | R2=0.3685 RMSE=32.848\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.6488 | R2=0.3043 RMSE=34.476\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.0896 | R2=0.5807 RMSE=26.764\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.0222 | R2=0.5436 RMSE=27.926\n",
            "[Epoch 09] s=0.00 topk=0 val=-0.6560 | R2=0.1965 RMSE=37.052\n",
            "[Epoch 10] s=0.00 topk=0 val=-0.9359 | R2=0.4450 RMSE=30.793\n",
            "[Epoch 11] s=0.00 topk=0 val=-0.4969 | R2=0.2563 RMSE=35.645\n",
            "[Epoch 12] s=0.00 topk=0 val=-1.0868 | R2=0.5786 RMSE=26.831\n",
            "[Epoch 13] s=0.05 topk=1 val=-0.9888 | R2=0.4901 RMSE=29.515\n",
            "[Epoch 14] s=0.10 topk=1 val=-1.0477 | R2=0.5623 RMSE=27.345\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.0026 | R2=0.5436 RMSE=27.926\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.1605 | R2=0.6136 RMSE=25.694\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.0797 | R2=0.5218 RMSE=28.582\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.0963 | R2=0.6343 RMSE=24.996\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.1775 | R2=0.6058 RMSE=25.950\n",
            "[Epoch 20] s=0.40 topk=1 val=-1.1902 | R2=0.6473 RMSE=24.549\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.1613 | R2=0.6429 RMSE=24.699\n",
            "[Epoch 22] s=0.50 topk=1 val=-0.2846 | R2=0.6165 RMSE=25.597\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.1890 | R2=0.6509 RMSE=24.422\n",
            "[Epoch 24] s=0.60 topk=1 val=-0.9060 | R2=0.5917 RMSE=26.412\n",
            "[Epoch 25] s=0.65 topk=1 val=-0.5192 | R2=0.2556 RMSE=35.662\n",
            "[Epoch 26] s=0.70 topk=1 val=-1.2248 | R2=0.6429 RMSE=24.700\n",
            "[Epoch 27] s=0.75 topk=1 val=-1.1727 | R2=0.6678 RMSE=23.824\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.1258 | R2=0.6611 RMSE=24.064\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.0022 | R2=0.5162 RMSE=28.751\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.2579 | R2=0.6564 RMSE=24.229\n",
            "[Epoch 31] s=0.95 topk=1 val=-0.8299 | R2=0.6589 RMSE=24.141\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.2288 | R2=0.6563 RMSE=24.233\n",
            "[Epoch 33] s=1.00 topk=1 val=-0.9181 | R2=0.6869 RMSE=23.130\n",
            "[Epoch 34] s=1.00 topk=1 val=-0.9093 | R2=0.5856 RMSE=26.610\n",
            "[Epoch 35] s=1.00 topk=1 val=-0.9995 | R2=0.5627 RMSE=27.333\n",
            "[Epoch 36] s=1.00 topk=1 val=-0.8064 | R2=0.5191 RMSE=28.664\n",
            "[Epoch 37] s=1.00 topk=1 val=-1.1876 | R2=0.6614 RMSE=24.051\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.1347 | R2=0.6747 RMSE=23.577\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.1462 | R2=0.6737 RMSE=23.611\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.2994 | R2=0.6925 RMSE=22.923\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.1046 | R2=0.5942 RMSE=26.330\n",
            "[Epoch 42] s=1.00 topk=1 val=-1.2381 | R2=0.6830 RMSE=23.271\n",
            "[Epoch 43] s=1.00 topk=1 val=-0.7700 | R2=0.5673 RMSE=27.190\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.1954 | R2=0.6725 RMSE=23.655\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.2268 | R2=0.6850 RMSE=23.198\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.1987 | R2=0.6986 RMSE=22.691\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.2169 | R2=0.6744 RMSE=23.586\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.1447 | R2=0.6832 RMSE=23.267\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.1267 | R2=0.6511 RMSE=24.414\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.2730 | R2=0.7050 RMSE=22.450\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.1766 | R2=0.6900 RMSE=23.013\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.2958 | R2=0.7038 RMSE=22.495\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.3235 | R2=0.7060 RMSE=22.412\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.0881 | R2=0.7050 RMSE=22.452\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.3010 | R2=0.7039 RMSE=22.493\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.2818 | R2=0.6959 RMSE=22.793\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.2599 | R2=0.6961 RMSE=22.787\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.0118 | R2=0.5758 RMSE=26.921\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.3178 | R2=0.7085 RMSE=22.317\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.2401 | R2=0.6936 RMSE=22.879\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.0947 | R2=0.6832 RMSE=23.266\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.2661 | R2=0.7132 RMSE=22.136\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.3513 | R2=0.7161 RMSE=22.023\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.1855 | R2=0.6706 RMSE=23.723\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.2958 | R2=0.7131 RMSE=22.140\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.3501 | R2=0.7249 RMSE=21.681\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.3131 | R2=0.7180 RMSE=21.950\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.2817 | R2=0.7069 RMSE=22.379\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.3159 | R2=0.7183 RMSE=21.937\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.2473 | R2=0.7065 RMSE=22.394\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.3131 | R2=0.7109 RMSE=22.223\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.3146 | R2=0.7091 RMSE=22.294\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.3086 | R2=0.7128 RMSE=22.150\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.3230 | R2=0.7166 RMSE=22.004\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.3076 | R2=0.7154 RMSE=22.052\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.3036 | R2=0.7148 RMSE=22.074\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.3039 | R2=0.7142 RMSE=22.097\n",
            "Early stopping (best val=-1.3513)\n",
            "\n",
            "==============================\n",
            " Target FD004 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.6883 RMSE=23.866 MAE=18.263\n",
            "UQ: 90% PI coverage=0.831 | mean width=58.100 | cal_scale=0.780\n",
            "Gate usage (avg weights): [0.315  0.217  0.1937 0.2744]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb3cuTS6kts1",
        "outputId": "5c77a7c5-ad4e-4117-86a1-20292393dd69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AbOTESvqktrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s8D_Tsmktob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAPO8bwMktmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVD-G6z1ktjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7SFP9T0ktge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXYmAZlgktdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}