{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyNDFW5pdXk9JnJIbmLj7N1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salehin555/MOE-RUL-Prediction/blob/main/RUL_PREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD001"
      ],
      "metadata": {
        "id": "j8dbLAMpsojL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD001\"]\n",
        "TARGETS = [\"FD001\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqWRossENQMO",
        "outputId": "9bff4e2f-dc21-440d-8499-f34ba515b920"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=0.1561 | R2=0.5270 RMSE=28.475\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.1708 | R2=0.6182 RMSE=25.583\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.3969 | R2=0.6275 RMSE=25.270\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.7160 | R2=0.7017 RMSE=22.614\n",
            "[Epoch 05] s=0.00 topk=0 val=-0.8247 | R2=0.5785 RMSE=26.882\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.7446 | R2=0.4484 RMSE=30.750\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.2552 | R2=0.7263 RMSE=21.662\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.3979 | R2=0.7389 RMSE=21.156\n",
            "[Epoch 09] s=0.00 topk=0 val=1.6415 | R2=-0.0068 RMSE=41.544\n",
            "[Epoch 10] s=0.00 topk=0 val=-1.1568 | R2=0.6992 RMSE=22.709\n",
            "[Epoch 11] s=0.00 topk=0 val=-1.0664 | R2=0.5933 RMSE=26.403\n",
            "[Epoch 12] s=0.00 topk=0 val=-1.1788 | R2=0.7430 RMSE=20.990\n",
            "[Epoch 13] s=0.05 topk=1 val=-1.4030 | R2=0.7206 RMSE=21.886\n",
            "[Epoch 14] s=0.10 topk=1 val=-0.3117 | R2=-0.1598 RMSE=44.588\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.3494 | R2=0.7115 RMSE=22.237\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.3317 | R2=0.6758 RMSE=23.575\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.0910 | R2=0.6094 RMSE=25.875\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.3731 | R2=0.7439 RMSE=20.952\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.4185 | R2=0.7631 RMSE=20.154\n",
            "[Epoch 20] s=0.40 topk=1 val=-1.3033 | R2=0.6481 RMSE=24.560\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.4330 | R2=0.7896 RMSE=18.990\n",
            "[Epoch 22] s=0.50 topk=1 val=-1.1973 | R2=0.6925 RMSE=22.960\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.4546 | R2=0.7709 RMSE=19.817\n",
            "[Epoch 24] s=0.60 topk=1 val=-1.5270 | R2=0.7869 RMSE=19.115\n",
            "[Epoch 25] s=0.65 topk=1 val=-1.3113 | R2=0.6797 RMSE=23.433\n",
            "[Epoch 26] s=0.70 topk=1 val=-1.2081 | R2=0.7059 RMSE=22.454\n",
            "[Epoch 27] s=0.75 topk=1 val=-1.3360 | R2=0.6925 RMSE=22.959\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.4625 | R2=0.7594 RMSE=20.309\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.4295 | R2=0.7512 RMSE=20.652\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.3847 | R2=0.7538 RMSE=20.545\n",
            "[Epoch 31] s=0.95 topk=1 val=-1.5255 | R2=0.7022 RMSE=22.593\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.5791 | R2=0.7304 RMSE=21.497\n",
            "[Epoch 33] s=1.00 topk=1 val=-1.5779 | R2=0.7823 RMSE=19.319\n",
            "[Epoch 34] s=1.00 topk=1 val=-1.4432 | R2=0.6956 RMSE=22.842\n",
            "[Epoch 35] s=1.00 topk=1 val=-1.0393 | R2=0.6836 RMSE=23.288\n",
            "[Epoch 36] s=1.00 topk=1 val=-1.4538 | R2=0.7446 RMSE=20.925\n",
            "[Epoch 37] s=1.00 topk=1 val=-1.6428 | R2=0.7645 RMSE=20.092\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.5717 | R2=0.7448 RMSE=20.918\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.5950 | R2=0.7333 RMSE=21.382\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.2931 | R2=0.7735 RMSE=19.704\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.3739 | R2=0.6719 RMSE=23.715\n",
            "[Epoch 42] s=1.00 topk=1 val=-1.4606 | R2=0.7297 RMSE=21.527\n",
            "[Epoch 43] s=1.00 topk=1 val=-1.3636 | R2=0.6139 RMSE=25.728\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.3144 | R2=0.7896 RMSE=18.993\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.5231 | R2=0.7601 RMSE=20.278\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.1633 | R2=0.6857 RMSE=23.210\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.5693 | R2=0.7410 RMSE=21.072\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.5969 | R2=0.7836 RMSE=19.262\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.3612 | R2=0.7961 RMSE=18.695\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.5803 | R2=0.7831 RMSE=19.284\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.6526 | R2=0.7791 RMSE=19.461\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.6530 | R2=0.7518 RMSE=20.629\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.6800 | R2=0.7599 RMSE=20.286\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.6087 | R2=0.6771 RMSE=23.529\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.4824 | R2=0.6874 RMSE=23.150\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.6999 | R2=0.7374 RMSE=21.218\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.6454 | R2=0.7531 RMSE=20.571\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.5887 | R2=0.7519 RMSE=20.625\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.6410 | R2=0.7619 RMSE=20.205\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.7122 | R2=0.7735 RMSE=19.706\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.6955 | R2=0.7517 RMSE=20.631\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.7319 | R2=0.7616 RMSE=20.217\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.6455 | R2=0.7364 RMSE=21.258\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.7263 | R2=0.7862 RMSE=19.145\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.6922 | R2=0.7959 RMSE=18.707\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.7307 | R2=0.7975 RMSE=18.630\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.7200 | R2=0.7865 RMSE=19.129\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.7467 | R2=0.7908 RMSE=18.935\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.7434 | R2=0.7883 RMSE=19.051\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.7289 | R2=0.7944 RMSE=18.773\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.7354 | R2=0.8004 RMSE=18.496\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.7410 | R2=0.7851 RMSE=19.192\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.6696 | R2=0.7781 RMSE=19.502\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.6934 | R2=0.7831 RMSE=19.283\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.7697 | R2=0.7903 RMSE=18.960\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.7611 | R2=0.7928 RMSE=18.845\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.7898 | R2=0.7945 RMSE=18.771\n",
            "[Epoch 78] s=1.00 topk=1 val=-1.7860 | R2=0.7921 RMSE=18.879\n",
            "[Epoch 79] s=1.00 topk=1 val=-1.7774 | R2=0.7917 RMSE=18.897\n",
            "[Epoch 80] s=1.00 topk=1 val=-1.7857 | R2=0.7896 RMSE=18.989\n",
            "[Epoch 81] s=1.00 topk=1 val=-1.7756 | R2=0.7859 RMSE=19.159\n",
            "[Epoch 82] s=1.00 topk=1 val=-1.7782 | R2=0.7882 RMSE=19.054\n",
            "[Epoch 83] s=1.00 topk=1 val=-1.7821 | R2=0.7835 RMSE=19.265\n",
            "[Epoch 84] s=1.00 topk=1 val=-1.7835 | R2=0.7906 RMSE=18.948\n",
            "[Epoch 85] s=1.00 topk=1 val=-1.7844 | R2=0.7894 RMSE=19.001\n",
            "\n",
            "==============================\n",
            " Target FD001 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.8602 RMSE=14.984 MAE=11.038\n",
            "UQ: 90% PI coverage=0.900 | mean width=49.980 | cal_scale=0.800\n",
            "Gate usage (avg weights): [0.4411 0.134  0.223  0.202 ]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oKpyAxUtNQJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rVRCmUrNQHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD002"
      ],
      "metadata": {
        "id": "w18ZsiaThpbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD002\"]\n",
        "TARGETS = [\"FD002\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO4SpMn7XzQ4",
        "outputId": "50c8bb40-62fb-47f2-c52b-e09997912eec"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=-0.2407 | R2=-0.0040 RMSE=41.841\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.5685 | R2=0.0405 RMSE=40.904\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.8230 | R2=0.5002 RMSE=29.521\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.8968 | R2=0.5992 RMSE=26.435\n",
            "[Epoch 05] s=0.00 topk=0 val=-0.8805 | R2=0.5406 RMSE=28.303\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.9473 | R2=0.5696 RMSE=27.395\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.1210 | R2=0.6504 RMSE=24.692\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.1902 | R2=0.7201 RMSE=22.093\n",
            "[Epoch 09] s=0.00 topk=0 val=-0.9997 | R2=0.4966 RMSE=29.627\n",
            "[Epoch 10] s=0.00 topk=0 val=-1.1631 | R2=0.6726 RMSE=23.893\n",
            "[Epoch 11] s=0.00 topk=0 val=-0.8876 | R2=0.4254 RMSE=31.653\n",
            "[Epoch 12] s=0.00 topk=0 val=-0.9271 | R2=0.4042 RMSE=32.233\n",
            "[Epoch 13] s=0.05 topk=1 val=-0.8902 | R2=0.6009 RMSE=26.379\n",
            "[Epoch 14] s=0.10 topk=1 val=-0.5136 | R2=0.7117 RMSE=22.419\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.1794 | R2=0.6934 RMSE=23.123\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.1896 | R2=0.7095 RMSE=22.508\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.1095 | R2=0.6102 RMSE=26.069\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.0835 | R2=0.6793 RMSE=23.647\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.1647 | R2=0.6793 RMSE=23.648\n",
            "[Epoch 20] s=0.40 topk=1 val=-0.7054 | R2=0.2853 RMSE=35.302\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.2020 | R2=0.7009 RMSE=22.837\n",
            "[Epoch 22] s=0.50 topk=1 val=-1.2153 | R2=0.7169 RMSE=22.220\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.0153 | R2=0.5870 RMSE=26.836\n",
            "[Epoch 24] s=0.60 topk=1 val=-1.2576 | R2=0.7014 RMSE=22.818\n",
            "[Epoch 25] s=0.65 topk=1 val=-1.3112 | R2=0.7348 RMSE=21.503\n",
            "[Epoch 26] s=0.70 topk=1 val=-0.9807 | R2=0.7528 RMSE=20.760\n",
            "[Epoch 27] s=0.75 topk=1 val=-0.9184 | R2=0.5170 RMSE=29.022\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.2618 | R2=0.7269 RMSE=21.823\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.0039 | R2=0.6235 RMSE=25.623\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.3316 | R2=0.7510 RMSE=20.838\n",
            "[Epoch 31] s=0.95 topk=1 val=-1.0646 | R2=0.5661 RMSE=27.505\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.2939 | R2=0.7387 RMSE=21.346\n",
            "[Epoch 33] s=1.00 topk=1 val=-1.2091 | R2=0.6180 RMSE=25.807\n",
            "[Epoch 34] s=1.00 topk=1 val=-1.2384 | R2=0.7222 RMSE=22.011\n",
            "[Epoch 35] s=1.00 topk=1 val=-1.3076 | R2=0.7428 RMSE=21.178\n",
            "[Epoch 36] s=1.00 topk=1 val=-1.2329 | R2=0.7287 RMSE=21.751\n",
            "[Epoch 37] s=1.00 topk=1 val=-0.7625 | R2=0.5482 RMSE=28.068\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.3025 | R2=0.7408 RMSE=21.260\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.0330 | R2=0.6239 RMSE=25.610\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.3740 | R2=0.7619 RMSE=20.375\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.3331 | R2=0.7556 RMSE=20.643\n",
            "[Epoch 42] s=1.00 topk=1 val=-0.9993 | R2=0.7386 RMSE=21.348\n",
            "[Epoch 43] s=1.00 topk=1 val=-1.3101 | R2=0.7572 RMSE=20.578\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.3435 | R2=0.7487 RMSE=20.933\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.2292 | R2=0.7172 RMSE=22.206\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.2736 | R2=0.7038 RMSE=22.727\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.2271 | R2=0.6859 RMSE=23.404\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.3371 | R2=0.7704 RMSE=20.009\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.1921 | R2=0.6959 RMSE=23.029\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.3824 | R2=0.7483 RMSE=20.952\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.2435 | R2=0.6419 RMSE=24.987\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.3536 | R2=0.7647 RMSE=20.257\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.3232 | R2=0.7417 RMSE=21.224\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.3653 | R2=0.7579 RMSE=20.547\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.2940 | R2=0.7140 RMSE=22.331\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.2491 | R2=0.7385 RMSE=21.352\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.3916 | R2=0.7645 RMSE=20.265\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.3706 | R2=0.7695 RMSE=20.046\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.3701 | R2=0.7552 RMSE=20.661\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.2851 | R2=0.7315 RMSE=21.638\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.3485 | R2=0.7504 RMSE=20.861\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.0526 | R2=0.7327 RMSE=21.589\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.3591 | R2=0.7569 RMSE=20.589\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.3811 | R2=0.7670 RMSE=20.155\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.3697 | R2=0.7767 RMSE=19.733\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.4193 | R2=0.7618 RMSE=20.379\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.4428 | R2=0.7772 RMSE=19.711\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.4294 | R2=0.7671 RMSE=20.153\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.3394 | R2=0.7391 RMSE=21.329\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.4267 | R2=0.7709 RMSE=19.987\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.4237 | R2=0.7731 RMSE=19.890\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.4297 | R2=0.7796 RMSE=19.604\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.4309 | R2=0.7774 RMSE=19.702\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.4117 | R2=0.7824 RMSE=19.477\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.4335 | R2=0.7763 RMSE=19.749\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.4179 | R2=0.7693 RMSE=20.055\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.4310 | R2=0.7731 RMSE=19.889\n",
            "[Epoch 78] s=1.00 topk=1 val=-1.4396 | R2=0.7810 RMSE=19.540\n",
            "[Epoch 79] s=1.00 topk=1 val=-1.4503 | R2=0.7823 RMSE=19.483\n",
            "[Epoch 80] s=1.00 topk=1 val=-1.4410 | R2=0.7856 RMSE=19.334\n",
            "[Epoch 81] s=1.00 topk=1 val=-1.4413 | R2=0.7819 RMSE=19.503\n",
            "[Epoch 82] s=1.00 topk=1 val=-1.4522 | R2=0.7831 RMSE=19.446\n",
            "[Epoch 83] s=1.00 topk=1 val=-1.4476 | R2=0.7816 RMSE=19.514\n",
            "[Epoch 84] s=1.00 topk=1 val=-1.4533 | R2=0.7823 RMSE=19.481\n",
            "[Epoch 85] s=1.00 topk=1 val=-1.4359 | R2=0.7831 RMSE=19.445\n",
            "\n",
            "==============================\n",
            " Target FD002 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.8102 RMSE=18.628 MAE=14.563\n",
            "UQ: 90% PI coverage=0.874 | mean width=50.521 | cal_scale=0.720\n",
            "Gate usage (avg weights): [0.1726 0.1753 0.3413 0.3108]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD003"
      ],
      "metadata": {
        "id": "FRjf7sTPsSEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD003\"]\n",
        "TARGETS = [\"FD003\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H-jVN79XzJG",
        "outputId": "e26ac17f-e694-4ea0-ba6e-70a3c2dbb69c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=0.0894 | R2=0.6722 RMSE=23.885\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.3108 | R2=0.5821 RMSE=26.968\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.6049 | R2=0.6585 RMSE=24.381\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.9182 | R2=0.6446 RMSE=24.870\n",
            "[Epoch 05] s=0.00 topk=0 val=-1.3225 | R2=0.6947 RMSE=23.051\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.8848 | R2=0.6259 RMSE=25.517\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.2841 | R2=0.7014 RMSE=22.798\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.0305 | R2=0.7097 RMSE=22.478\n",
            "[Epoch 09] s=0.00 topk=0 val=-1.4801 | R2=0.7613 RMSE=20.381\n",
            "[Epoch 10] s=0.00 topk=0 val=-1.5606 | R2=0.7627 RMSE=20.325\n",
            "[Epoch 11] s=0.00 topk=0 val=-1.3519 | R2=0.7336 RMSE=21.531\n",
            "[Epoch 12] s=0.00 topk=0 val=-1.1461 | R2=0.6607 RMSE=24.303\n",
            "[Epoch 13] s=0.05 topk=1 val=-1.5128 | R2=0.8100 RMSE=18.187\n",
            "[Epoch 14] s=0.10 topk=1 val=-1.5152 | R2=0.7922 RMSE=19.018\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.0954 | R2=0.6798 RMSE=23.606\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.1003 | R2=0.6635 RMSE=24.200\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.4028 | R2=0.7300 RMSE=21.677\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.3958 | R2=0.7589 RMSE=20.485\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.6033 | R2=0.8125 RMSE=18.066\n",
            "[Epoch 20] s=0.40 topk=1 val=-1.3196 | R2=0.7734 RMSE=19.858\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.3083 | R2=0.7524 RMSE=20.760\n",
            "[Epoch 22] s=0.50 topk=1 val=-1.2262 | R2=0.7267 RMSE=21.812\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.1892 | R2=0.6974 RMSE=22.949\n",
            "[Epoch 24] s=0.60 topk=1 val=-1.3303 | R2=0.7883 RMSE=19.197\n",
            "[Epoch 25] s=0.65 topk=1 val=-1.5635 | R2=0.8291 RMSE=17.245\n",
            "[Epoch 26] s=0.70 topk=1 val=-1.6173 | R2=0.8263 RMSE=17.390\n",
            "[Epoch 27] s=0.75 topk=1 val=-1.3369 | R2=0.7830 RMSE=19.433\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.4471 | R2=0.7734 RMSE=19.859\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.4173 | R2=0.7795 RMSE=19.592\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.6158 | R2=0.8034 RMSE=18.500\n",
            "[Epoch 31] s=0.95 topk=1 val=-1.3012 | R2=0.7349 RMSE=21.480\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.5673 | R2=0.8217 RMSE=17.616\n",
            "[Epoch 33] s=1.00 topk=1 val=-1.4981 | R2=0.8258 RMSE=17.415\n",
            "[Epoch 34] s=1.00 topk=1 val=-1.5747 | R2=0.8374 RMSE=16.822\n",
            "[Epoch 35] s=1.00 topk=1 val=-1.3560 | R2=0.7902 RMSE=19.107\n",
            "[Epoch 36] s=1.00 topk=1 val=-1.5591 | R2=0.8169 RMSE=17.851\n",
            "[Epoch 37] s=1.00 topk=1 val=-1.6562 | R2=0.8566 RMSE=15.798\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.4798 | R2=0.8219 RMSE=17.606\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.5767 | R2=0.8403 RMSE=16.674\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.5006 | R2=0.8381 RMSE=16.789\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.7389 | R2=0.8431 RMSE=16.523\n",
            "[Epoch 42] s=1.00 topk=1 val=-1.4606 | R2=0.7799 RMSE=19.574\n",
            "[Epoch 43] s=1.00 topk=1 val=-1.5675 | R2=0.8301 RMSE=17.197\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.6000 | R2=0.8304 RMSE=17.181\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.6743 | R2=0.8323 RMSE=17.084\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.6420 | R2=0.8452 RMSE=16.413\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.5966 | R2=0.8458 RMSE=16.382\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.6119 | R2=0.8379 RMSE=16.799\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.5619 | R2=0.8392 RMSE=16.729\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.5228 | R2=0.8283 RMSE=17.288\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.7279 | R2=0.8647 RMSE=15.347\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.6573 | R2=0.8438 RMSE=16.491\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.7599 | R2=0.8492 RMSE=16.201\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.7744 | R2=0.8504 RMSE=16.135\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.6478 | R2=0.8596 RMSE=15.630\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.7448 | R2=0.8581 RMSE=15.715\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.7589 | R2=0.8573 RMSE=15.758\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.7816 | R2=0.8639 RMSE=15.393\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.7314 | R2=0.8517 RMSE=16.067\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.4106 | R2=0.7940 RMSE=18.933\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.7136 | R2=0.8543 RMSE=15.925\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.7533 | R2=0.8479 RMSE=16.268\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.5139 | R2=0.8401 RMSE=16.681\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.5633 | R2=0.8314 RMSE=17.128\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.7390 | R2=0.8499 RMSE=16.163\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.7458 | R2=0.8537 RMSE=15.958\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.6545 | R2=0.8509 RMSE=16.111\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.7131 | R2=0.8534 RMSE=15.975\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.7869 | R2=0.8566 RMSE=15.798\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.8149 | R2=0.8599 RMSE=15.616\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.7777 | R2=0.8610 RMSE=15.556\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.5871 | R2=0.8592 RMSE=15.656\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.4918 | R2=0.8614 RMSE=15.533\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.4946 | R2=0.8597 RMSE=15.624\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.5769 | R2=0.8595 RMSE=15.638\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.6496 | R2=0.8605 RMSE=15.582\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.7048 | R2=0.8597 RMSE=15.625\n",
            "[Epoch 78] s=1.00 topk=1 val=-1.7199 | R2=0.8626 RMSE=15.467\n",
            "[Epoch 79] s=1.00 topk=1 val=-1.7372 | R2=0.8655 RMSE=15.301\n",
            "[Epoch 80] s=1.00 topk=1 val=-1.7432 | R2=0.8619 RMSE=15.505\n",
            "[Epoch 81] s=1.00 topk=1 val=-1.7909 | R2=0.8626 RMSE=15.466\n",
            "[Epoch 82] s=1.00 topk=1 val=-1.7916 | R2=0.8658 RMSE=15.283\n",
            "[Epoch 83] s=1.00 topk=1 val=-1.7761 | R2=0.8613 RMSE=15.536\n",
            "[Epoch 84] s=1.00 topk=1 val=-1.7710 | R2=0.8661 RMSE=15.266\n",
            "Early stopping (best val=-1.8149)\n",
            "\n",
            "==============================\n",
            " Target FD003 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.8035 RMSE=17.364 MAE=13.016\n",
            "UQ: 90% PI coverage=0.860 | mean width=41.859 | cal_scale=0.700\n",
            "Gate usage (avg weights): [0.3608 0.3335 0.1128 0.1929]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FD004"
      ],
      "metadata": {
        "id": "JZ9O7acDsUcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Interpretable MOE for C-MAPSS RUL (ONE-CELL Colab)\n",
        "# Fixes:\n",
        "#  - Noisy Top-K gating + load balancing + entropy bonus (Sub-Obj 1)\n",
        "#  - Physics constraints: monotonic + smooth (Sub-Obj 2)\n",
        "#  - UQ: NLL + MC Dropout + PI calibration (Sub-Obj 3)\n",
        "#  - Transfer-ready structure (Sub-Obj 4 hooks; single-domain run here)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"\n",
        "PRETRAIN_DOMAINS = [\"FD004\"]\n",
        "TARGETS = [\"FD004\"]\n",
        "DO_FINETUNE = False\n",
        "ENSEMBLE_SIZE = 1\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # in normalized y scale\n",
        "\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx]), self.domain_ids[idx]\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "# ✅ FIXED predict_mc: safe unpacking for 7-value forward()\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, n_mc, batch_size=256):\n",
        "    model.train()\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32),batch_size=batch_size,shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i) for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare domain\n",
        "# ---------------------------\n",
        "def prepare_domain(fd, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr=add_rul_train(tr,cfg.max_rul)\n",
        "    te=add_rul_test(te,rul,cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv>cfg.low_var_thresh]\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, n_show=1024):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    out = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "def train_on_domain(domain):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=np.zeros(len(X_tr),dtype=int))\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va,domain_ids=np.zeros(len(X_va),dtype=int))\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1,cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "        for xb,yb,ub,cb,db in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val loss + quick metrics\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb,db in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        yhat_val, _ = predict_mc(model, X_va, n_mc=1)\n",
        "        if cfg.normalize_y:\n",
        "            yhat_val = yhat_val*cfg.max_rul\n",
        "            y_val_raw = y_va*cfg.max_rul\n",
        "        else:\n",
        "            y_val_raw = y_va\n",
        "        r2 = r2_score(y_val_raw, yhat_val)\n",
        "        rmse = math.sqrt(mean_squared_error(y_val_raw, yhat_val))\n",
        "        print(f\"[Epoch {ep:02d}] s={s:.2f} topk={int(use_topk)} val={va_loss:.4f} | R2={r2:.4f} RMSE={rmse:.3f}\")\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "for fd in set(PRETRAIN_DOMAINS + TARGETS):\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        ensure_exists(os.path.join(DATA_DIR, f))\n",
        "\n",
        "domains={}\n",
        "scaler=None\n",
        "kept=None\n",
        "for fd in PRETRAIN_DOMAINS:\n",
        "    domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "    scaler=domains[fd][\"scaler\"]\n",
        "    kept=domains[fd][\"kept_cols\"]\n",
        "for fd in TARGETS:\n",
        "    if fd not in domains:\n",
        "        domains[fd]=prepare_domain(fd, scaler=scaler, kept_cols=kept)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_on_domain(domains[PRETRAIN_DOMAINS[0]]))\n",
        "\n",
        "for fd in TARGETS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    X_te, y_te, *_ = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(models[0], X_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(models[0], X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "\n",
        "    gate_stats(models[0], X_te)\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTyhYaTuktyH",
        "outputId": "bee81858-ff8c-407a-e7fb-c1176fc639f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training model 1/1\n",
            "==============================\n",
            "[Epoch 01] s=0.00 topk=0 val=-0.1370 | R2=-0.0879 RMSE=43.112\n",
            "[Epoch 02] s=0.00 topk=0 val=-0.4181 | R2=-0.0584 RMSE=42.523\n",
            "[Epoch 03] s=0.00 topk=0 val=-0.6923 | R2=0.5067 RMSE=29.030\n",
            "[Epoch 04] s=0.00 topk=0 val=-0.7007 | R2=0.6014 RMSE=26.095\n",
            "[Epoch 05] s=0.00 topk=0 val=-0.7130 | R2=0.3685 RMSE=32.848\n",
            "[Epoch 06] s=0.00 topk=0 val=-0.6488 | R2=0.3043 RMSE=34.476\n",
            "[Epoch 07] s=0.00 topk=0 val=-1.0896 | R2=0.5807 RMSE=26.764\n",
            "[Epoch 08] s=0.00 topk=0 val=-1.0222 | R2=0.5436 RMSE=27.926\n",
            "[Epoch 09] s=0.00 topk=0 val=-0.6560 | R2=0.1965 RMSE=37.052\n",
            "[Epoch 10] s=0.00 topk=0 val=-0.9359 | R2=0.4450 RMSE=30.793\n",
            "[Epoch 11] s=0.00 topk=0 val=-0.4969 | R2=0.2563 RMSE=35.645\n",
            "[Epoch 12] s=0.00 topk=0 val=-1.0868 | R2=0.5786 RMSE=26.831\n",
            "[Epoch 13] s=0.05 topk=1 val=-0.9888 | R2=0.4901 RMSE=29.515\n",
            "[Epoch 14] s=0.10 topk=1 val=-1.0477 | R2=0.5623 RMSE=27.345\n",
            "[Epoch 15] s=0.15 topk=1 val=-1.0026 | R2=0.5436 RMSE=27.926\n",
            "[Epoch 16] s=0.20 topk=1 val=-1.1605 | R2=0.6136 RMSE=25.694\n",
            "[Epoch 17] s=0.25 topk=1 val=-1.0797 | R2=0.5218 RMSE=28.582\n",
            "[Epoch 18] s=0.30 topk=1 val=-1.0963 | R2=0.6343 RMSE=24.996\n",
            "[Epoch 19] s=0.35 topk=1 val=-1.1775 | R2=0.6058 RMSE=25.950\n",
            "[Epoch 20] s=0.40 topk=1 val=-1.1902 | R2=0.6473 RMSE=24.549\n",
            "[Epoch 21] s=0.45 topk=1 val=-1.1613 | R2=0.6429 RMSE=24.699\n",
            "[Epoch 22] s=0.50 topk=1 val=-0.2846 | R2=0.6165 RMSE=25.597\n",
            "[Epoch 23] s=0.55 topk=1 val=-1.1890 | R2=0.6509 RMSE=24.422\n",
            "[Epoch 24] s=0.60 topk=1 val=-0.9060 | R2=0.5917 RMSE=26.412\n",
            "[Epoch 25] s=0.65 topk=1 val=-0.5192 | R2=0.2556 RMSE=35.662\n",
            "[Epoch 26] s=0.70 topk=1 val=-1.2248 | R2=0.6429 RMSE=24.700\n",
            "[Epoch 27] s=0.75 topk=1 val=-1.1727 | R2=0.6678 RMSE=23.824\n",
            "[Epoch 28] s=0.80 topk=1 val=-1.1258 | R2=0.6611 RMSE=24.064\n",
            "[Epoch 29] s=0.85 topk=1 val=-1.0022 | R2=0.5162 RMSE=28.751\n",
            "[Epoch 30] s=0.90 topk=1 val=-1.2579 | R2=0.6564 RMSE=24.229\n",
            "[Epoch 31] s=0.95 topk=1 val=-0.8299 | R2=0.6589 RMSE=24.141\n",
            "[Epoch 32] s=1.00 topk=1 val=-1.2288 | R2=0.6563 RMSE=24.233\n",
            "[Epoch 33] s=1.00 topk=1 val=-0.9181 | R2=0.6869 RMSE=23.130\n",
            "[Epoch 34] s=1.00 topk=1 val=-0.9093 | R2=0.5856 RMSE=26.610\n",
            "[Epoch 35] s=1.00 topk=1 val=-0.9995 | R2=0.5627 RMSE=27.333\n",
            "[Epoch 36] s=1.00 topk=1 val=-0.8064 | R2=0.5191 RMSE=28.664\n",
            "[Epoch 37] s=1.00 topk=1 val=-1.1876 | R2=0.6614 RMSE=24.051\n",
            "[Epoch 38] s=1.00 topk=1 val=-1.1347 | R2=0.6747 RMSE=23.577\n",
            "[Epoch 39] s=1.00 topk=1 val=-1.1462 | R2=0.6737 RMSE=23.611\n",
            "[Epoch 40] s=1.00 topk=1 val=-1.2994 | R2=0.6925 RMSE=22.923\n",
            "[Epoch 41] s=1.00 topk=1 val=-1.1046 | R2=0.5942 RMSE=26.330\n",
            "[Epoch 42] s=1.00 topk=1 val=-1.2381 | R2=0.6830 RMSE=23.271\n",
            "[Epoch 43] s=1.00 topk=1 val=-0.7700 | R2=0.5673 RMSE=27.190\n",
            "[Epoch 44] s=1.00 topk=1 val=-1.1954 | R2=0.6725 RMSE=23.655\n",
            "[Epoch 45] s=1.00 topk=1 val=-1.2268 | R2=0.6850 RMSE=23.198\n",
            "[Epoch 46] s=1.00 topk=1 val=-1.1987 | R2=0.6986 RMSE=22.691\n",
            "[Epoch 47] s=1.00 topk=1 val=-1.2169 | R2=0.6744 RMSE=23.586\n",
            "[Epoch 48] s=1.00 topk=1 val=-1.1447 | R2=0.6832 RMSE=23.267\n",
            "[Epoch 49] s=1.00 topk=1 val=-1.1267 | R2=0.6511 RMSE=24.414\n",
            "[Epoch 50] s=1.00 topk=1 val=-1.2730 | R2=0.7050 RMSE=22.450\n",
            "[Epoch 51] s=1.00 topk=1 val=-1.1766 | R2=0.6900 RMSE=23.013\n",
            "[Epoch 52] s=1.00 topk=1 val=-1.2958 | R2=0.7038 RMSE=22.495\n",
            "[Epoch 53] s=1.00 topk=1 val=-1.3235 | R2=0.7060 RMSE=22.412\n",
            "[Epoch 54] s=1.00 topk=1 val=-1.0881 | R2=0.7050 RMSE=22.452\n",
            "[Epoch 55] s=1.00 topk=1 val=-1.3010 | R2=0.7039 RMSE=22.493\n",
            "[Epoch 56] s=1.00 topk=1 val=-1.2818 | R2=0.6959 RMSE=22.793\n",
            "[Epoch 57] s=1.00 topk=1 val=-1.2599 | R2=0.6961 RMSE=22.787\n",
            "[Epoch 58] s=1.00 topk=1 val=-1.0118 | R2=0.5758 RMSE=26.921\n",
            "[Epoch 59] s=1.00 topk=1 val=-1.3178 | R2=0.7085 RMSE=22.317\n",
            "[Epoch 60] s=1.00 topk=1 val=-1.2401 | R2=0.6936 RMSE=22.879\n",
            "[Epoch 61] s=1.00 topk=1 val=-1.0947 | R2=0.6832 RMSE=23.266\n",
            "[Epoch 62] s=1.00 topk=1 val=-1.2661 | R2=0.7132 RMSE=22.136\n",
            "[Epoch 63] s=1.00 topk=1 val=-1.3513 | R2=0.7161 RMSE=22.023\n",
            "[Epoch 64] s=1.00 topk=1 val=-1.1855 | R2=0.6706 RMSE=23.723\n",
            "[Epoch 65] s=1.00 topk=1 val=-1.2958 | R2=0.7131 RMSE=22.140\n",
            "[Epoch 66] s=1.00 topk=1 val=-1.3501 | R2=0.7249 RMSE=21.681\n",
            "[Epoch 67] s=1.00 topk=1 val=-1.3131 | R2=0.7180 RMSE=21.950\n",
            "[Epoch 68] s=1.00 topk=1 val=-1.2817 | R2=0.7069 RMSE=22.379\n",
            "[Epoch 69] s=1.00 topk=1 val=-1.3159 | R2=0.7183 RMSE=21.937\n",
            "[Epoch 70] s=1.00 topk=1 val=-1.2473 | R2=0.7065 RMSE=22.394\n",
            "[Epoch 71] s=1.00 topk=1 val=-1.3131 | R2=0.7109 RMSE=22.223\n",
            "[Epoch 72] s=1.00 topk=1 val=-1.3146 | R2=0.7091 RMSE=22.294\n",
            "[Epoch 73] s=1.00 topk=1 val=-1.3086 | R2=0.7128 RMSE=22.150\n",
            "[Epoch 74] s=1.00 topk=1 val=-1.3230 | R2=0.7166 RMSE=22.004\n",
            "[Epoch 75] s=1.00 topk=1 val=-1.3076 | R2=0.7154 RMSE=22.052\n",
            "[Epoch 76] s=1.00 topk=1 val=-1.3036 | R2=0.7148 RMSE=22.074\n",
            "[Epoch 77] s=1.00 topk=1 val=-1.3039 | R2=0.7142 RMSE=22.097\n",
            "Early stopping (best val=-1.3513)\n",
            "\n",
            "==============================\n",
            " Target FD004 | LAST window per unit\n",
            "==============================\n",
            "Point: R2=0.6883 RMSE=23.866 MAE=18.263\n",
            "UQ: 90% PI coverage=0.831 | mean width=58.100 | cal_scale=0.780\n",
            "Gate usage (avg weights): [0.315  0.217  0.1937 0.2744]\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb3cuTS6kts1",
        "outputId": "5c77a7c5-ad4e-4117-86a1-20292393dd69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL Generalized Interpretable MoE for C-MAPSS RUL (ONE CELL)\n",
        "#  - One joint model for FD001..FD004 (best generalization)\n",
        "#  - Global scaler on all domains (no FD001 anchoring)\n",
        "#  - Domain-balanced contiguous batching\n",
        "#  - Regime embedding from (op1, op2, op3)\n",
        "#  - GroupNorm + FiLM conditioning for domain/regime shift\n",
        "#  - MoE Top-K gating + load balance + entropy\n",
        "#  - Physics constraints: monotonic + smooth\n",
        "#  - UQ: Gaussian NLL + MC Dropout + PI calibration\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"   # change if needed\n",
        "FDS = [\"FD001\",\"FD002\",\"FD003\",\"FD004\"]\n",
        "ENSEMBLE_SIZE = 1       # keep 1 for \"single best model\"\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # windows\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    # model\n",
        "    n_experts: int = 6\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 224\n",
        "    head_hidden: int = 224\n",
        "    dropout: float = 0.12\n",
        "    gate_dropout: float = 0.08\n",
        "\n",
        "    # gating schedule\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.10\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    # training\n",
        "    epochs: int = 110\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 16\n",
        "\n",
        "    # losses\n",
        "    aux_mse_weight: float = 0.50\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # normalized y scale\n",
        "\n",
        "    # physics\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "\n",
        "    # moe regularizers\n",
        "    lambda_lb: float = 0.30\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    # ramp\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    # RUL focus (helps FD004)\n",
        "    rul_focus: float = 0.7   # 0..1; higher => focus on low RUL region\n",
        "\n",
        "    # UQ\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "# Regime key/id (from ops)\n",
        "# ---------------------------\n",
        "def compute_regime_key(df: pd.DataFrame):\n",
        "    # robust discrete key across FD001..FD004:\n",
        "    # op1: round to nearest int (FD001 ~0 -> 0, FD002/4 -> 10/20/35/42)\n",
        "    # op2: round op2*100 (0.25->25, 0.70->70, 0.84->84, FD001 ~0 -> 0)\n",
        "    # op3: int (60, 100)\n",
        "    a = np.round(df[\"op1\"].values).astype(int)\n",
        "    b = np.round(df[\"op2\"].values * 100).astype(int)\n",
        "    c = df[\"op3\"].values.astype(int)\n",
        "    return list(zip(a,b,c))\n",
        "\n",
        "def build_regime_vocab(all_train_dfs: Dict[str,pd.DataFrame]):\n",
        "    keys = []\n",
        "    for fd, df in all_train_dfs.items():\n",
        "        keys.extend(compute_regime_key(df))\n",
        "    uniq = sorted(set(keys))\n",
        "    key2id = {k:i for i,k in enumerate(uniq)}\n",
        "    return key2id\n",
        "\n",
        "def attach_regime_id(df: pd.DataFrame, key2id: Dict[Tuple[int,int,int],int]):\n",
        "    df = df.copy()\n",
        "    keys = compute_regime_key(df)\n",
        "    # if unseen keys appear in test, add them (rare but safe)\n",
        "    for k in keys:\n",
        "        if k not in key2id:\n",
        "            key2id[k] = len(key2id)\n",
        "    df[\"regime_id\"] = [key2id[k] for k in keys]\n",
        "    return df, key2id\n",
        "\n",
        "# ---------------------------\n",
        "# Windowing\n",
        "# ---------------------------\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles, regimes = [], [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "        reg   = g[\"regime_id\"].values.astype(np.int64)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "            regimes.append(reg[end])  # last-step regime\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles), np.array(regimes)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles, regimes):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx], regimes[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + domain-balanced contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles, domain_ids, regime_ids):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "        self.domain_ids = torch.tensor(domain_ids, dtype=torch.long)\n",
        "        self.regime_ids = torch.tensor(regime_ids, dtype=torch.long)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X[idx],\n",
        "                self.y[idx],\n",
        "                int(self.unit_ids[idx]),\n",
        "                int(self.cycles[idx]),\n",
        "                self.domain_ids[idx],\n",
        "                self.regime_ids[idx])\n",
        "\n",
        "class DomainBalancedContiguousSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Each batch: pick engines_per_batch engines total, balanced across domains.\n",
        "    For each engine: sample a contiguous block_len segment (like your original sampler).\n",
        "    \"\"\"\n",
        "    def __init__(self, unit_ids, cycles, domain_ids, batch_size, block_len, n_domains, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.domain_ids=np.array(domain_ids)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.n_domains=n_domains\n",
        "\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "        # build engine index lists per domain: (domain, unit) -> sorted sample indices\n",
        "        self.domain_units = {d: [] for d in range(n_domains)}\n",
        "        self.engine_to_sorted = {}  # (d,u)-> idx array sorted by cycle\n",
        "\n",
        "        for d in range(n_domains):\n",
        "            mask_d = self.domain_ids == d\n",
        "            units_d = np.unique(self.unit_ids[mask_d])\n",
        "            self.domain_units[d] = units_d.tolist()\n",
        "            for u in units_d:\n",
        "                idx = np.where(mask_d & (self.unit_ids==u))[0]\n",
        "                idx = idx[np.argsort(self.cycles[idx])]\n",
        "                self.engine_to_sorted[(d,u)] = idx\n",
        "\n",
        "        # handle domains with few units\n",
        "        self.domain_units = {d: np.array(v, dtype=int) for d,v in self.domain_units.items()}\n",
        "\n",
        "    def __iter__(self):\n",
        "        # prepare per-domain engine order\n",
        "        dom_orders = {}\n",
        "        dom_ptr = {}\n",
        "        for d in range(self.n_domains):\n",
        "            units = self.domain_units[d].copy()\n",
        "            if len(units)==0:\n",
        "                continue\n",
        "            if self.shuffle:\n",
        "                np.random.shuffle(units)\n",
        "            dom_orders[d] = units\n",
        "            dom_ptr[d] = 0\n",
        "\n",
        "        # how many engines per domain each batch\n",
        "        base = self.engines_per_batch // self.n_domains\n",
        "        rem  = self.engines_per_batch % self.n_domains\n",
        "        engines_per_dom = {d: base + (1 if d < rem else 0) for d in range(self.n_domains)}\n",
        "\n",
        "        # generate batches\n",
        "        batch=[]\n",
        "        while True:\n",
        "            any_added = False\n",
        "            for d in range(self.n_domains):\n",
        "                need = engines_per_dom[d]\n",
        "                if need <= 0: continue\n",
        "                if d not in dom_orders or len(dom_orders[d])==0:\n",
        "                    continue\n",
        "                for _ in range(need):\n",
        "                    # cycle through engines if we run out\n",
        "                    if dom_ptr[d] >= len(dom_orders[d]):\n",
        "                        dom_ptr[d] = 0\n",
        "                        if self.shuffle:\n",
        "                            np.random.shuffle(dom_orders[d])\n",
        "                    u = int(dom_orders[d][dom_ptr[d]])\n",
        "                    dom_ptr[d] += 1\n",
        "\n",
        "                    idx = self.engine_to_sorted[(d,u)]\n",
        "                    L = len(idx)\n",
        "                    if L <= self.block_len:\n",
        "                        take = np.random.choice(idx, size=self.block_len, replace=True)\n",
        "                    else:\n",
        "                        s = np.random.randint(0, L-self.block_len)\n",
        "                        take = idx[s:s+self.block_len]\n",
        "                    batch.extend(take.tolist())\n",
        "                    any_added = True\n",
        "\n",
        "                    if len(batch) >= self.batch_size:\n",
        "                        yield batch[:self.batch_size]\n",
        "                        batch=[]\n",
        "\n",
        "            if not any_added:\n",
        "                break\n",
        "\n",
        "        if batch:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        # rough estimate\n",
        "        n_eng = sum(len(self.domain_units[d]) for d in range(self.n_domains))\n",
        "        if n_eng == 0: return 0\n",
        "        return max(1, math.ceil(n_eng / max(1,self.engines_per_batch)))\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)   # RUL should decrease over time\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# Gating + load balance\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(96,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_mc(model, X, domain_ids, regime_ids, n_mc, batch_size=256):\n",
        "    model.train()  # enable dropout for MC\n",
        "    dl=DataLoader(\n",
        "        list(zip(torch.tensor(X,dtype=torch.float32),\n",
        "                 torch.tensor(domain_ids,dtype=torch.long),\n",
        "                 torch.tensor(regime_ids,dtype=torch.long))),\n",
        "        batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb, db, rb in dl:\n",
        "        xb=xb.to(DEVICE); db=db.to(DEVICE); rb=rb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            out = model(xb, db, rb, use_topk=True, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mean, log_var = out[0], out[1]\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(log_var))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, d_val, r_val, y_val_raw, alpha, n_mc, max_over=0.01):\n",
        "    mean, var = predict_mc(model, X_val, d_val, r_val, n_mc=n_mc)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Model (GroupNorm + FiLM conditioning + Regime/Domain emb)\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.gn1=nn.GroupNorm(8,96)   # more domain-robust than BatchNorm\n",
        "        self.gn2=nn.GroupNorm(8,96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.silu(self.gn1(self.conv1(x)))\n",
        "        x=F.silu(self.gn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class FiLMConditioner(nn.Module):\n",
        "    def __init__(self, z_dim, op_dim, dom_emb_dim, reg_emb_dim):\n",
        "        super().__init__()\n",
        "        in_dim = op_dim + dom_emb_dim + reg_emb_dim\n",
        "        h = max(96, z_dim//2)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, h),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(h, 2*z_dim)\n",
        "        )\n",
        "    def forward(self, op_stats, dom_emb, reg_emb):\n",
        "        x = torch.cat([op_stats, dom_emb, reg_emb], dim=-1)\n",
        "        gb = self.mlp(x)\n",
        "        gamma, beta = gb.chunk(2, dim=-1)\n",
        "        # stable scaling\n",
        "        gamma = 0.1 * torch.tanh(gamma)\n",
        "        beta  = 0.1 * torch.tanh(beta)\n",
        "        return gamma, beta\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id, extra_dim):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(96,hidden//2)\n",
        "        drop  = min(0.35, base_dropout + 0.03*expert_id)\n",
        "        self.expert_emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim + extra_dim + 16, width), nn.SiLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(64,width//2)), nn.SiLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(64,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self, z, extra):\n",
        "        B=z.size(0)\n",
        "        e=self.expert_emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z, extra, e], dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features, n_domains, n_regimes,\n",
        "                 dom_emb_dim=8, reg_emb_dim=8):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features, cfg.enc_hidden, cfg.dropout)\n",
        "        self.domain_emb = nn.Embedding(n_domains, dom_emb_dim)\n",
        "        self.regime_emb = nn.Embedding(n_regimes, reg_emb_dim)\n",
        "\n",
        "        self.film = FiLMConditioner(cfg.enc_hidden, op_dim=9, dom_emb_dim=dom_emb_dim, reg_emb_dim=reg_emb_dim)\n",
        "\n",
        "        extra_dim = dom_emb_dim + reg_emb_dim\n",
        "        self.experts = nn.ModuleList([\n",
        "            ExpertHead(cfg.enc_hidden, cfg.head_hidden, cfg.dropout, i, extra_dim=extra_dim)\n",
        "            for i in range(cfg.n_experts)\n",
        "        ])\n",
        "\n",
        "        gate_in_dim = cfg.enc_hidden + 9 + extra_dim\n",
        "        self.gate = GatingNet(gate_in_dim, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, domain_id, regime_id, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "\n",
        "        # op stats (last, mean, std)\n",
        "        op = x[:, :, :3]\n",
        "        op_stats = torch.cat([op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)  # (B,9)\n",
        "\n",
        "        dom_e = self.domain_emb(domain_id)\n",
        "        reg_e = self.regime_emb(regime_id)\n",
        "\n",
        "        gamma, beta = self.film(op_stats, dom_e, reg_e)\n",
        "        zc = z * (1.0 + gamma) + beta\n",
        "\n",
        "        extra = torch.cat([dom_e, reg_e], dim=-1)\n",
        "        gate_in = torch.cat([zc, op_stats, extra], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(zc, extra)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        # expert diversity (light)\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Prepare ALL domains (global scaler, global regime vocab)\n",
        "# ---------------------------\n",
        "def prepare_all_domains(data_dir: str, fds: List[str]):\n",
        "    # load all train dfs first (for global regime vocab)\n",
        "    train_dfs = {}\n",
        "    raw_splits = {}\n",
        "    for fd in fds:\n",
        "        tr, te, rul = load_cmapss_split(data_dir, fd)\n",
        "        tr = add_rul_train(tr, cfg.max_rul)\n",
        "        te = add_rul_test(te, rul, cfg.max_rul)\n",
        "        train_dfs[fd] = tr\n",
        "        raw_splits[fd] = (tr, te)\n",
        "\n",
        "    key2id = build_regime_vocab(train_dfs)\n",
        "    domains = {}\n",
        "\n",
        "    feature_cols = base_feature_columns()  # keep all sensors always\n",
        "\n",
        "    # attach regime id for each domain\n",
        "    for di, fd in enumerate(fds):\n",
        "        tr, te = raw_splits[fd]\n",
        "        tr, key2id = attach_regime_id(tr, key2id)\n",
        "        te, key2id = attach_regime_id(te, key2id)\n",
        "\n",
        "        X_tr_all, y_tr_all, u_tr_all, c_tr_all, r_tr_all = make_windows(tr, cfg.window, cfg.stride, feature_cols)\n",
        "        X_te_all, y_te_all, u_te_all, c_te_all, r_te_all = make_windows(te, cfg.window, cfg.stride, feature_cols)\n",
        "        X_te_last, y_te_last, u_te_last, c_te_last, r_te_last = last_window_per_unit(\n",
        "            X_te_all, y_te_all, u_te_all, c_te_all, r_te_all\n",
        "        )\n",
        "\n",
        "        domains[fd] = {\n",
        "            \"domain_id\": di,\n",
        "            \"feature_cols\": feature_cols,\n",
        "            \"train_all\": (X_tr_all, y_tr_all, u_tr_all, c_tr_all, r_tr_all),\n",
        "            \"test_last_raw\": (X_te_last, y_te_last, u_te_last, c_te_last, r_te_last)\n",
        "        }\n",
        "\n",
        "    # global scaler on ALL TRAIN windows of ALL domains\n",
        "    scaler = StandardScaler()\n",
        "    X_stack = []\n",
        "    for fd in fds:\n",
        "        X_tr_all = domains[fd][\"train_all\"][0]\n",
        "        X_stack.append(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "    X_stack = np.concatenate(X_stack, axis=0)\n",
        "    scaler.fit(X_stack)\n",
        "\n",
        "    # apply scaler + split into train/val by units per domain\n",
        "    for fd in fds:\n",
        "        X_tr_all, y_tr_all, u_tr_all, c_tr_all, r_tr_all = domains[fd][\"train_all\"]\n",
        "        X_te_last, y_te_last, u_te_last, c_te_last, r_te_last = domains[fd][\"test_last_raw\"]\n",
        "\n",
        "        X_tr_all = scaler.transform(X_tr_all.reshape(-1, X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "        X_te_last = scaler.transform(X_te_last.reshape(-1, X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "        tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "\n",
        "        X_tr, y_tr, u_tr, c_tr, r_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx], r_tr_all[tr_idx]\n",
        "        X_va, y_va, u_va, c_va, r_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx], r_tr_all[va_idx]\n",
        "\n",
        "        y_te_raw = y_te_last.copy()\n",
        "\n",
        "        if cfg.normalize_y:\n",
        "            y_tr = y_tr / cfg.max_rul\n",
        "            y_va = y_va / cfg.max_rul\n",
        "            y_te = y_te_last / cfg.max_rul\n",
        "        else:\n",
        "            y_te = y_te_last\n",
        "\n",
        "        d_tr = np.full(len(X_tr), domains[fd][\"domain_id\"], dtype=np.int64)\n",
        "        d_va = np.full(len(X_va), domains[fd][\"domain_id\"], dtype=np.int64)\n",
        "        d_te = np.full(len(X_te_last), domains[fd][\"domain_id\"], dtype=np.int64)\n",
        "\n",
        "        domains[fd].update({\n",
        "            \"scaler\": scaler,\n",
        "            \"n_regimes\": len(key2id),\n",
        "            \"train\": (X_tr, y_tr, u_tr, c_tr, d_tr, r_tr),\n",
        "            \"val\": (X_va, y_va, u_va, c_va, d_va, r_va),\n",
        "            \"test_last\": (X_te_last, y_te, u_te_last, c_te_last, d_te, r_te_last),\n",
        "            \"test_last_y_raw\": y_te_raw\n",
        "        })\n",
        "\n",
        "    return domains, scaler, len(key2id)\n",
        "\n",
        "# ---------------------------\n",
        "# Interpretability\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def gate_stats(model, X, d, r, n_show=2048):\n",
        "    model.eval()\n",
        "    xb=torch.tensor(X[:n_show],dtype=torch.float32).to(DEVICE)\n",
        "    db=torch.tensor(d[:n_show],dtype=torch.long).to(DEVICE)\n",
        "    rb=torch.tensor(r[:n_show],dtype=torch.long).to(DEVICE)\n",
        "    out = model(xb, db, rb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "    w = out[2].cpu().numpy()\n",
        "    print(\"Gate usage (avg weights):\", np.round(w.mean(axis=0),4))\n",
        "\n",
        "# ---------------------------\n",
        "# Training (ONE joint model)\n",
        "# ---------------------------\n",
        "def train_joint(domains: Dict[str,dict], n_domains: int, n_regimes: int):\n",
        "    # build combined train set\n",
        "    X_tr=[]; y_tr=[]; u_tr=[]; c_tr=[]; d_tr=[]; r_tr=[]\n",
        "    X_va_by_fd={}; y_va_by_fd={}; d_va_by_fd={}; r_va_by_fd={}; u_va_by_fd={}; c_va_by_fd={}\n",
        "\n",
        "    for fd, dom in domains.items():\n",
        "        X,y,u,c,d,r = dom[\"train\"]\n",
        "        X_tr.append(X); y_tr.append(y); u_tr.append(u); c_tr.append(c); d_tr.append(d); r_tr.append(r)\n",
        "\n",
        "        Xv,yv,uv,cv,dv,rv = dom[\"val\"]\n",
        "        X_va_by_fd[fd]=Xv; y_va_by_fd[fd]=yv; d_va_by_fd[fd]=dv; r_va_by_fd[fd]=rv\n",
        "        u_va_by_fd[fd]=uv; c_va_by_fd[fd]=cv\n",
        "\n",
        "    X_tr=np.concatenate(X_tr); y_tr=np.concatenate(y_tr)\n",
        "    u_tr=np.concatenate(u_tr); c_tr=np.concatenate(c_tr)\n",
        "    d_tr=np.concatenate(d_tr); r_tr=np.concatenate(r_tr)\n",
        "\n",
        "    model = InterpretableMoE(n_features=X_tr.shape[-1], n_domains=n_domains, n_regimes=n_regimes).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr,domain_ids=d_tr, regime_ids=r_tr)\n",
        "\n",
        "    sampler = DomainBalancedContiguousSampler(\n",
        "        ds_tr.unit_ids, ds_tr.cycles, ds_tr.domain_ids.numpy(),\n",
        "        cfg.batch_size, cfg.block_len, n_domains=n_domains, shuffle=True\n",
        "    )\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "\n",
        "    # val loaders per domain\n",
        "    val_loaders={}\n",
        "    for fd in domains.keys():\n",
        "        Xv=y_va_by_fd[fd]*0  # dummy init\n",
        "        ds = RULWindowDataset(\n",
        "            X_va_by_fd[fd], y_va_by_fd[fd],\n",
        "            u_va_by_fd[fd], c_va_by_fd[fd],\n",
        "            domain_ids=d_va_by_fd[fd], regime_ids=r_va_by_fd[fd]\n",
        "        )\n",
        "        val_loaders[fd]=DataLoader(ds, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1, cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "        use_topk = (ep > cfg.warmup_epochs)\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s)\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s)\n",
        "\n",
        "        mono_w   = cfg.lambda_mono*s\n",
        "        smooth_w = cfg.lambda_smooth*s\n",
        "        lb_w     = cfg.lambda_lb*s\n",
        "        ent_w    = cfg.lambda_ent*s\n",
        "        div_w    = cfg.lambda_div*s\n",
        "        dead_w   = cfg.lambda_dead*s\n",
        "\n",
        "        model.train()\n",
        "        tr_loss=0.0\n",
        "\n",
        "        for xb,yb,ub,cb,db,rb in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            db=db.to(DEVICE); rb=rb.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, db, rb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            # emphasize low RUL region (helps FD004)\n",
        "            # yb normalized in [0,1]; low RUL => small yb\n",
        "            weight = 1.0 + cfg.rul_focus * (1.0 - yb)\n",
        "            nll = (gaussian_nll(yb, mean, logv) * weight).mean()\n",
        "            mse = (F.mse_loss(mean, yb, reduction=\"none\") * weight).mean()\n",
        "            hub = (F.huber_loss(mean, yb, delta=cfg.huber_delta, reduction=\"none\") * weight).mean()\n",
        "\n",
        "            mono, smooth = physics_losses(mean, ub, cb)\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "            tr_loss += float(loss.detach().cpu())\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # ---- validation: average across domains (key for generalization)\n",
        "        model.eval()\n",
        "        val_total=0.0\n",
        "        val_rmse={}\n",
        "        with torch.no_grad():\n",
        "            for fd, dl_va in val_loaders.items():\n",
        "                va_loss=0.0\n",
        "                y_true=[]; y_hat=[]\n",
        "                for xb,yb,ub,cb,db,rb in dl_va:\n",
        "                    xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                    db=db.to(DEVICE); rb=rb.to(DEVICE)\n",
        "                    mean, logv, *_ = model(xb, db, rb, use_topk=True, noise_std=0.0, temperature=1.0, train=False)\n",
        "\n",
        "                    # same weighted objective for selection\n",
        "                    weight = 1.0 + cfg.rul_focus * (1.0 - yb)\n",
        "                    nll = (gaussian_nll(yb, mean, logv) * weight).mean()\n",
        "                    mse = (F.mse_loss(mean, yb, reduction=\"none\") * weight).mean()\n",
        "                    hub = (F.huber_loss(mean, yb, delta=cfg.huber_delta, reduction=\"none\") * weight).mean()\n",
        "                    va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "\n",
        "                    y_true.append(yb.detach().cpu().numpy())\n",
        "                    y_hat.append(mean.detach().cpu().numpy())\n",
        "\n",
        "                va_loss /= max(1,len(dl_va))\n",
        "                val_total += va_loss\n",
        "\n",
        "                yt = np.concatenate(y_true)\n",
        "                yh = np.concatenate(y_hat)\n",
        "                if cfg.normalize_y:\n",
        "                    yt = yt*cfg.max_rul\n",
        "                    yh = yh*cfg.max_rul\n",
        "                val_rmse[fd] = math.sqrt(mean_squared_error(yt, yh))\n",
        "\n",
        "        val_total /= max(1, len(val_loaders))\n",
        "        rmse_str = \" | \".join([f\"{fd}:{val_rmse[fd]:.2f}\" for fd in FDS])\n",
        "        print(f\"[Epoch {ep:03d}] s={s:.2f} topk={int(use_topk)} val_avg={val_total:.4f} | valRMSE {rmse_str}\")\n",
        "\n",
        "        if val_total + 1e-6 < best_val:\n",
        "            best_val=val_total\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                print(f\"Early stopping (best avg val={best_val:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# EVALUATION\n",
        "# ---------------------------\n",
        "def eval_domain(model, domains, fd):\n",
        "    X_te, y_te, u_te, c_te, d_te, r_te = domains[fd][\"test_last\"]\n",
        "    y_te_raw = domains[fd][\"test_last_y_raw\"]\n",
        "\n",
        "    X_va, y_va, u_va, c_va, d_va, r_va = domains[fd][\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    mean, var = predict_mc(model, X_te, d_te, r_te, n_mc=cfg.mc_samples)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    # PI calibration per-domain (optional; doesn't change point accuracy)\n",
        "    scale = calibrate_scale_min_width(model, X_va, d_va, r_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, max_over=0.01)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    r2 = r2_score(y_te_raw, mean_raw)\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    mae = mean_absolute_error(y_te_raw, mean_raw)\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "\n",
        "    print(f\"Point: R2={r2:.4f} RMSE={rmse:.3f} MAE={mae:.3f}\")\n",
        "    print(f\"UQ: {int((1-cfg.pi_alpha)*100)}% PI coverage={cov:.3f} | mean width={(hi-lo).mean():.3f} | cal_scale={scale:.3f}\")\n",
        "    gate_stats(model, X_te, d_te, r_te)\n",
        "\n",
        "# ---------------------------\n",
        "# RUN\n",
        "# ---------------------------\n",
        "# auto-fallback for local testing (optional)\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    DATA_DIR = \"/mnt/data\"\n",
        "\n",
        "# file checks\n",
        "missing=[]\n",
        "for fd in FDS:\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        if not os.path.exists(os.path.join(DATA_DIR, f)):\n",
        "            missing.append(os.path.join(DATA_DIR,f))\n",
        "if missing:\n",
        "    raise FileNotFoundError(\"Missing required C-MAPSS files:\\n\" + \"\\n\".join(missing))\n",
        "\n",
        "domains, scaler, n_regimes = prepare_all_domains(DATA_DIR, FDS)\n",
        "n_domains = len(FDS)\n",
        "\n",
        "models=[]\n",
        "for mi in range(ENSEMBLE_SIZE):\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Training JOINT model {mi+1}/{ENSEMBLE_SIZE}\")\n",
        "    print(\"==============================\")\n",
        "    set_seed(cfg.seed + mi)\n",
        "    models.append(train_joint(domains, n_domains=n_domains, n_regimes=n_regimes))\n",
        "\n",
        "model = models[0]\n",
        "\n",
        "for fd in FDS:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\" Target {fd} | LAST window per unit\")\n",
        "    print(\"==============================\")\n",
        "    eval_domain(model, domains, fd)\n",
        "\n",
        "print(\"\\n✅ Done. One generalized model trained on FD001..FD004.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "AbOTESvqktrA",
        "outputId": "6a3ef73c-603c-4f81-961f-0a2585f91f6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Training JOINT model 1/1\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2325982094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==============================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_domains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_domains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_regimes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_regimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2325982094.py\u001b[0m in \u001b[0;36mtrain_joint\u001b[0;34m(domains, n_domains, n_regimes)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmono_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmono\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmooth_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmooth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlb_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ment_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiv_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdead_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s8D_Tsmktob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# END-TO-END: Baseline vs MoE Ablation Table for C-MAPSS (One Cell)\n",
        "# Outputs: RMSE table for FD001..FD004 + Avg RMSE\n",
        "# Variants:\n",
        "#  1) Single Expert\n",
        "#  2) Dense MoE (no top-k)\n",
        "#  3) MoE + Top-K\n",
        "#  4) MoE + Top-K + Physics\n",
        "#  5) Full (Top-K + Physics + UQ + Calib)\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# USER SETTINGS\n",
        "# ---------------------------\n",
        "DATA_DIR = \"/content\"          # change if needed\n",
        "FDS = [\"FD001\",\"FD002\",\"FD003\",\"FD004\"]\n",
        "ENSEMBLE_SIZE = 1             # keep 1 for table fairness\n",
        "FAST_DEV_RUN = False          # True = fewer epochs for quick check\n",
        "PRINT_UQ_FOR_FULL = True      # prints PI coverage/width for Full variant\n",
        "\n",
        "# ---------------------------\n",
        "# Install deps\n",
        "# ---------------------------\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n",
        "\n",
        "_ensure(\"scikit-learn\", \"sklearn\")\n",
        "_ensure(\"scipy\", \"scipy\")\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import norm\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    window: int = 30\n",
        "    stride: int = 1\n",
        "    max_rul: int = 125\n",
        "    normalize_y: bool = True\n",
        "    val_ratio_units: float = 0.2\n",
        "    seed: int = 42\n",
        "\n",
        "    # feature filter (leave as-is; it only drops near-constant columns)\n",
        "    drop_low_var: bool = True\n",
        "    low_var_thresh: float = 1e-6\n",
        "\n",
        "    # MoE\n",
        "    n_experts: int = 4\n",
        "    top_k: int = 2\n",
        "    enc_hidden: int = 192\n",
        "    head_hidden: int = 192\n",
        "    dropout: float = 0.08\n",
        "    gate_dropout: float = 0.05\n",
        "\n",
        "    # gating schedule\n",
        "    gate_noise_max: float = 1.0\n",
        "    gate_noise_min: float = 0.15\n",
        "    temp_max: float = 2.0\n",
        "    temp_min: float = 0.7\n",
        "\n",
        "    # training\n",
        "    epochs: int = 85\n",
        "    batch_size: int = 128\n",
        "    block_len: int = 12\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    early_stop_patience: int = 14\n",
        "\n",
        "    # loss mix\n",
        "    aux_mse_weight: float = 0.55\n",
        "    huber_weight: float = 0.15\n",
        "    huber_delta: float = 0.08  # normalized y scale\n",
        "\n",
        "    # physics (used only for physics variants)\n",
        "    lambda_mono: float = 0.10\n",
        "    lambda_smooth: float = 0.03\n",
        "\n",
        "    # MoE regularizers (used only for MoE variants)\n",
        "    lambda_lb: float = 0.25\n",
        "    lambda_ent: float = 0.02\n",
        "    lambda_div: float = 0.01\n",
        "    lambda_dead: float = 0.06\n",
        "    dead_floor: float = 0.03\n",
        "\n",
        "    # ramp\n",
        "    warmup_epochs: int = 12\n",
        "    ramp_epochs: int = 20\n",
        "\n",
        "    # UQ\n",
        "    mc_samples: int = 30\n",
        "    pi_alpha: float = 0.10  # 90% PI\n",
        "\n",
        "cfg = CFG()\n",
        "if FAST_DEV_RUN:\n",
        "    cfg.epochs = 12\n",
        "    cfg.warmup_epochs = 3\n",
        "    cfg.ramp_epochs = 4\n",
        "    cfg.early_stop_patience = 4\n",
        "    cfg.mc_samples = 10\n",
        "\n",
        "# ---------------------------\n",
        "# Utils\n",
        "# ---------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "def ensure_exists(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "\n",
        "def base_feature_columns() -> List[str]:\n",
        "    return [\"op1\", \"op2\", \"op3\"] + [f\"s{i}\" for i in range(1, 22)]\n",
        "\n",
        "def ramp(ep, warmup, ramp_len):\n",
        "    if ep <= warmup: return 0.0\n",
        "    return float(min(1.0, (ep - warmup) / max(1, ramp_len)))\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    return a + (b - a) * t\n",
        "\n",
        "# ---------------------------\n",
        "# C-MAPSS load + labels\n",
        "# ---------------------------\n",
        "def load_cmapss_split(data_dir: str, fd: str):\n",
        "    train_file = os.path.join(data_dir, f\"train_{fd}.txt\")\n",
        "    test_file  = os.path.join(data_dir, f\"test_{fd}.txt\")\n",
        "    rul_file   = os.path.join(data_dir, f\"RUL_{fd}.txt\")\n",
        "    ensure_exists(train_file); ensure_exists(test_file); ensure_exists(rul_file)\n",
        "\n",
        "    train_df = pd.read_csv(train_file, sep=r\"\\s+\", header=None)\n",
        "    test_df  = pd.read_csv(test_file,  sep=r\"\\s+\", header=None)\n",
        "    rul_df   = pd.read_csv(rul_file,   sep=r\"\\s+\", header=None)\n",
        "\n",
        "    cols = [\"unit\",\"cycle\",\"op1\",\"op2\",\"op3\"] + [f\"s{i}\" for i in range(1,22)]\n",
        "    train_df.columns = cols\n",
        "    test_df.columns  = cols\n",
        "    rul_df.columns   = [\"RUL_last\"]\n",
        "    return train_df, test_df, rul_df\n",
        "\n",
        "def add_rul_train(df: pd.DataFrame, max_rul: int):\n",
        "    df = df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    df[\"RUL\"] = df.apply(lambda r: max_cycle.loc[r[\"unit\"]] - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "def add_rul_test(test_df: pd.DataFrame, rul_df: pd.DataFrame, max_rul: int):\n",
        "    df = test_df.copy()\n",
        "    max_cycle = df.groupby(\"unit\")[\"cycle\"].max()\n",
        "    rul_last = rul_df[\"RUL_last\"].values\n",
        "    mapping = {u: rul_last[u-1] for u in sorted(df[\"unit\"].unique())}\n",
        "    df[\"RUL\"] = df.apply(lambda r: (max_cycle.loc[r[\"unit\"]] + mapping[r[\"unit\"]]) - r[\"cycle\"], axis=1)\n",
        "    df[\"RUL\"] = df[\"RUL\"].clip(upper=max_rul)\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "# Windowing\n",
        "# ---------------------------\n",
        "def make_windows(df, window, stride, feature_cols, target_col=\"RUL\"):\n",
        "    xs, ys, units, cycles = [], [], [], []\n",
        "    for unit, g in df.groupby(\"unit\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        feats = g[feature_cols].values.astype(np.float32)\n",
        "        targ  = g[target_col].values.astype(np.float32)\n",
        "        cyc   = g[\"cycle\"].values.astype(np.int32)\n",
        "\n",
        "        for end in range(window-1, len(g), stride):\n",
        "            start = end-window+1\n",
        "            xs.append(feats[start:end+1])\n",
        "            ys.append(targ[end])\n",
        "            units.append(unit)\n",
        "            cycles.append(cyc[end])\n",
        "    return np.stack(xs), np.array(ys), np.array(units), np.array(cycles)\n",
        "\n",
        "def last_window_per_unit(X, y, unit_ids, cycles):\n",
        "    idx=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        m = unit_ids==u\n",
        "        i = np.argmax(cycles[m])\n",
        "        idx.append(np.where(m)[0][i])\n",
        "    idx=np.array(idx)\n",
        "    return X[idx], y[idx], unit_ids[idx], cycles[idx]\n",
        "\n",
        "def split_by_units(unit_ids, val_ratio, seed):\n",
        "    rng=np.random.default_rng(seed)\n",
        "    units=np.unique(unit_ids)\n",
        "    rng.shuffle(units)\n",
        "    n_val=max(1,int(len(units)*val_ratio))\n",
        "    val=set(units[:n_val].tolist())\n",
        "    tr=np.array([i for i,u in enumerate(unit_ids) if u not in val])\n",
        "    va=np.array([i for i,u in enumerate(unit_ids) if u in val])\n",
        "    return tr, va\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset + contiguous sampler\n",
        "# ---------------------------\n",
        "class RULWindowDataset(Dataset):\n",
        "    def __init__(self, X, y, unit_ids, cycles):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.unit_ids = np.array(unit_ids)\n",
        "        self.cycles = np.array(cycles)\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], int(self.unit_ids[idx]), int(self.cycles[idx])\n",
        "\n",
        "class ContiguousEngineBatchSampler(Sampler):\n",
        "    def __init__(self, unit_ids, cycles, batch_size, block_len, shuffle=True):\n",
        "        self.unit_ids=np.array(unit_ids)\n",
        "        self.cycles=np.array(cycles)\n",
        "        self.batch_size=batch_size\n",
        "        self.block_len=block_len\n",
        "        self.shuffle=shuffle\n",
        "        self.units=np.unique(self.unit_ids)\n",
        "        self.unit_to_sorted={}\n",
        "        for u in self.units:\n",
        "            idx=np.where(self.unit_ids==u)[0]\n",
        "            idx=idx[np.argsort(self.cycles[idx])]\n",
        "            self.unit_to_sorted[u]=idx\n",
        "        self.engines_per_batch=max(1,batch_size//block_len)\n",
        "\n",
        "    def __iter__(self):\n",
        "        units=self.units.copy()\n",
        "        if self.shuffle: np.random.shuffle(units)\n",
        "        batch=[]\n",
        "        for u in units:\n",
        "            idx=self.unit_to_sorted[u]\n",
        "            L=len(idx)\n",
        "            if L<=self.block_len:\n",
        "                take=np.random.choice(idx,size=self.block_len,replace=True)\n",
        "            else:\n",
        "                s=np.random.randint(0,L-self.block_len)\n",
        "                take=idx[s:s+self.block_len]\n",
        "            batch.extend(take.tolist())\n",
        "            if len(batch)>=self.engines_per_batch*self.block_len:\n",
        "                yield batch[:self.batch_size]\n",
        "                batch=[]\n",
        "        if batch: yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.units)/self.engines_per_batch)\n",
        "\n",
        "# ---------------------------\n",
        "# Physics losses\n",
        "# ---------------------------\n",
        "def physics_losses(pred_mean, unit_ids, cycles, margin=0.0):\n",
        "    unit_ids=np.array(unit_ids); cycles=np.array(cycles)\n",
        "    mono_terms=[]; smooth_terms=[]\n",
        "    for u in np.unique(unit_ids):\n",
        "        idx=np.where(unit_ids==u)[0]\n",
        "        if len(idx)<2: continue\n",
        "        ord_idx=idx[np.argsort(cycles[idx])]\n",
        "        p=pred_mean[ord_idx]\n",
        "        mono=F.relu(p[1:]-p[:-1]+margin)\n",
        "        mono_terms.append(mono.mean())\n",
        "        if len(ord_idx)>=3:\n",
        "            second=p[2:]-2*p[1:-1]+p[:-2]\n",
        "            smooth_terms.append(torch.abs(second).mean())\n",
        "    mono_loss=torch.stack(mono_terms).mean() if mono_terms else pred_mean.new_tensor(0.)\n",
        "    smooth_loss=torch.stack(smooth_terms).mean() if smooth_terms else pred_mean.new_tensor(0.)\n",
        "    return mono_loss, smooth_loss\n",
        "\n",
        "# ---------------------------\n",
        "# MoE utilities\n",
        "# ---------------------------\n",
        "class GatingNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_experts, gate_dropout):\n",
        "        super().__init__()\n",
        "        h=max(64,in_dim//2)\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim,h),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(gate_dropout),\n",
        "            nn.Linear(h,n_experts),\n",
        "        )\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "def sparse_topk_softmax(logits, k, temperature):\n",
        "    logits = logits / max(1e-6, temperature)\n",
        "    topk_vals, topk_idx = torch.topk(logits, k=k, dim=-1)\n",
        "    masked = torch.full_like(logits, float(\"-inf\"))\n",
        "    masked.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
        "    w = F.softmax(masked, dim=-1)\n",
        "    return w, topk_idx\n",
        "\n",
        "def switch_load_balance_loss(w, top1_idx):\n",
        "    B,E=w.shape\n",
        "    importance=w.sum(dim=0)/(B+1e-8)\n",
        "    load=torch.bincount(top1_idx,minlength=E).float().to(w.device)/(B+1e-8)\n",
        "    return E*torch.sum(importance*load)\n",
        "\n",
        "def dead_expert_penalty(w, floor):\n",
        "    avg = w.mean(dim=0)\n",
        "    return F.relu(floor - avg).mean()\n",
        "\n",
        "# ---------------------------\n",
        "# UQ + calibration\n",
        "# ---------------------------\n",
        "def gaussian_nll(y, mean, log_var):\n",
        "    return 0.5*(torch.exp(-log_var)*(y-mean)**2 + log_var)\n",
        "\n",
        "def prediction_interval(mean, var, alpha):\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    std=np.sqrt(np.maximum(var,1e-8))\n",
        "    return mean - z*std, mean + z*std\n",
        "\n",
        "def coverage(y, lo, hi):\n",
        "    return float(np.mean((y>=lo)&(y<=hi)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_mc_mean_var(model, X, n_mc, batch_size=256, use_topk=True):\n",
        "    model.train()  # enable dropout\n",
        "    dl=DataLoader(torch.tensor(X,dtype=torch.float32), batch_size=batch_size, shuffle=False)\n",
        "    means_all=[]; vars_all=[]\n",
        "    for xb in dl:\n",
        "        xb=xb.to(DEVICE)\n",
        "        mc_m=[]; mc_v=[]\n",
        "        for _ in range(n_mc):\n",
        "            mean, logv, *_ = model(xb, use_topk=use_topk, noise_std=0.0, temperature=1.0, train=True)\n",
        "            mc_m.append(mean)\n",
        "            mc_v.append(torch.exp(logv))\n",
        "        mc_m=torch.stack(mc_m,dim=0)\n",
        "        mc_v=torch.stack(mc_v,dim=0)\n",
        "        mean_pred=mc_m.mean(dim=0)\n",
        "        epistemic=mc_m.var(dim=0,unbiased=False)\n",
        "        aleatoric=mc_v.mean(dim=0)\n",
        "        total=aleatoric+epistemic\n",
        "        means_all.append(mean_pred.cpu().numpy())\n",
        "        vars_all.append(total.cpu().numpy())\n",
        "    return np.concatenate(means_all), np.concatenate(vars_all)\n",
        "\n",
        "def calibrate_scale_min_width(model, X_val, y_val_raw, alpha, n_mc, max_over=0.01, use_topk=True):\n",
        "    mean, var = predict_mc_mean_var(model, X_val, n_mc=n_mc, use_topk=use_topk)\n",
        "    if cfg.normalize_y:\n",
        "        mean = mean * cfg.max_rul\n",
        "        var  = var  * (cfg.max_rul**2)\n",
        "    std = np.sqrt(np.maximum(var,1e-8))\n",
        "    z = norm.ppf(1-alpha/2)\n",
        "    target = 1-alpha\n",
        "    scales = np.linspace(0.6, 3.0, 121)\n",
        "    best=None\n",
        "    for s in scales:\n",
        "        lo = mean - z*s*std\n",
        "        hi = mean + z*s*std\n",
        "        cov = np.mean((y_val_raw>=lo)&(y_val_raw<=hi))\n",
        "        if cov >= (target - max_over):\n",
        "            best=s\n",
        "            break\n",
        "    return float(best if best is not None else scales[-1])\n",
        "\n",
        "# ---------------------------\n",
        "# Models\n",
        "# ---------------------------\n",
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv1d(n_features,96,3,padding=1)\n",
        "        self.conv2=nn.Conv1d(96,96,3,padding=1)\n",
        "        self.bn1=nn.BatchNorm1d(96)\n",
        "        self.bn2=nn.BatchNorm1d(96)\n",
        "        self.gru=nn.GRU(96,hidden,batch_first=True)\n",
        "        self.drop=nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x=x.transpose(1,2)\n",
        "        x=F.relu(self.bn1(self.conv1(x)))\n",
        "        x=F.relu(self.bn2(self.conv2(x)))\n",
        "        x=x.transpose(1,2)\n",
        "        _,h=self.gru(x)\n",
        "        return self.drop(h[-1])\n",
        "\n",
        "class ExpertHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, base_dropout, expert_id):\n",
        "        super().__init__()\n",
        "        width = hidden if expert_id%2==0 else max(64,hidden//2)\n",
        "        drop  = min(0.30, base_dropout + 0.03*expert_id)\n",
        "        self.emb = nn.Parameter(torch.randn(16)*0.02)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim+16, width), nn.ReLU(), nn.Dropout(drop),\n",
        "            nn.Linear(width, max(32,width//2)), nn.ReLU(), nn.Dropout(drop),\n",
        "        )\n",
        "        out=max(32,width//2)\n",
        "        self.mu=nn.Linear(out,1)\n",
        "        self.logv=nn.Linear(out,1)\n",
        "    def forward(self,z):\n",
        "        B=z.size(0)\n",
        "        e=self.emb.unsqueeze(0).expand(B,-1)\n",
        "        h=self.net(torch.cat([z,e],dim=-1))\n",
        "        mu=self.mu(h).squeeze(-1)\n",
        "        logv=self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "        return mu, logv\n",
        "\n",
        "class InterpretableMoE(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        self.experts = nn.ModuleList([ExpertHead(cfg.enc_hidden,cfg.head_hidden,cfg.dropout,i)\n",
        "                                      for i in range(cfg.n_experts)])\n",
        "        self.gate = GatingNet(cfg.enc_hidden+9, cfg.n_experts, cfg.gate_dropout)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        op = x[:,:, :3]\n",
        "        gate_in = torch.cat([z, op[:,-1,:], op.mean(dim=1), op.std(dim=1)], dim=-1)\n",
        "\n",
        "        logits = self.gate(gate_in)\n",
        "        if train and noise_std>0:\n",
        "            logits = logits + torch.randn_like(logits)*noise_std\n",
        "\n",
        "        if use_topk:\n",
        "            w, topk_idx = sparse_topk_softmax(logits, cfg.top_k, temperature)\n",
        "            top1 = topk_idx[:,0]\n",
        "        else:\n",
        "            w = F.softmax(logits/max(1e-6,temperature), dim=-1)\n",
        "            top1 = torch.argmax(w, dim=-1)\n",
        "\n",
        "        mus=[]; vars_=[]\n",
        "        for ex in self.experts:\n",
        "            mu, logv = ex(z)\n",
        "            mus.append(mu); vars_.append(torch.exp(logv))\n",
        "        mus=torch.stack(mus,dim=-1)\n",
        "        vars_=torch.stack(vars_,dim=-1)\n",
        "\n",
        "        mean=torch.sum(w*mus,dim=-1)\n",
        "        second=torch.sum(w*(vars_+mus**2),dim=-1)\n",
        "        var=(second-mean**2).clamp_min(1e-6)\n",
        "        log_var=torch.log(var)\n",
        "\n",
        "        lb = switch_load_balance_loss(w, top1)\n",
        "        ent = -(w*torch.log(w+1e-8)).sum(dim=-1).mean()\n",
        "        dead = dead_expert_penalty(w, cfg.dead_floor)\n",
        "\n",
        "        div=0.0\n",
        "        E=mus.shape[-1]\n",
        "        for i in range(E):\n",
        "            for j in range(i+1,E):\n",
        "                a=mus[:,i]-mus[:,i].mean()\n",
        "                b=mus[:,j]-mus[:,j].mean()\n",
        "                div += torch.abs(F.cosine_similarity(a.unsqueeze(-1), b.unsqueeze(-1), dim=-1)).mean()\n",
        "        div = div / max(1,(E*(E-1)//2))\n",
        "\n",
        "        return mean, log_var, w, lb, ent, div, dead\n",
        "\n",
        "class SingleExpertBaseline(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline: same encoder, one probabilistic head (mu, log_var).\n",
        "    To keep training code unified, it returns 7 values like MoE but with zeros.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.enc = SharedEncoder(n_features,cfg.enc_hidden,cfg.dropout)\n",
        "        h = cfg.head_hidden\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(cfg.enc_hidden, h), nn.ReLU(), nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(h, max(32,h//2)), nn.ReLU(), nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        out = max(32,h//2)\n",
        "        self.mu = nn.Linear(out, 1)\n",
        "        self.logv = nn.Linear(out, 1)\n",
        "\n",
        "    def forward(self, x, *, use_topk, noise_std, temperature, train):\n",
        "        z = self.enc(x)\n",
        "        h = self.net(z)\n",
        "        mu = self.mu(h).squeeze(-1)\n",
        "        logv = self.logv(h).squeeze(-1).clamp(-9,4)\n",
        "\n",
        "        # placeholders\n",
        "        w = torch.zeros((x.size(0), cfg.n_experts), device=x.device)\n",
        "        lb = mu.new_tensor(0.0)\n",
        "        ent = mu.new_tensor(0.0)\n",
        "        div = mu.new_tensor(0.0)\n",
        "        dead = mu.new_tensor(0.0)\n",
        "        return mu, logv, w, lb, ent, div, dead\n",
        "\n",
        "# ---------------------------\n",
        "# Domain preparation (single FD)\n",
        "# ---------------------------\n",
        "def prepare_domain(fd: str, scaler=None, kept_cols=None):\n",
        "    tr, te, rul = load_cmapss_split(DATA_DIR, fd)\n",
        "    tr = add_rul_train(tr, cfg.max_rul)\n",
        "    te = add_rul_test(te, rul, cfg.max_rul)\n",
        "\n",
        "    all_cols = base_feature_columns()\n",
        "    if kept_cols is None:\n",
        "        kept_cols = all_cols\n",
        "        if cfg.drop_low_var:\n",
        "            v = tr[all_cols].var(axis=0).values\n",
        "            kept_cols = [c for c,vv in zip(all_cols,v) if vv > cfg.low_var_thresh]\n",
        "            # always keep operating settings\n",
        "            for c in [\"op1\",\"op2\",\"op3\"]:\n",
        "                if c not in kept_cols:\n",
        "                    kept_cols = [\"op1\",\"op2\",\"op3\"] + [x for x in kept_cols if x not in [\"op1\",\"op2\",\"op3\"]]\n",
        "\n",
        "    X_tr_all, y_tr_all, u_tr_all, c_tr_all = make_windows(tr, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_all, y_te_all, u_te_all, c_te_all = make_windows(te, cfg.window, cfg.stride, kept_cols)\n",
        "    X_te_last, y_te_last, u_te_last, c_te_last = last_window_per_unit(X_te_all, y_te_all, u_te_all, c_te_all)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_tr_all.reshape(-1, X_tr_all.shape[-1]))\n",
        "\n",
        "    X_tr_all = scaler.transform(X_tr_all.reshape(-1,X_tr_all.shape[-1])).reshape(X_tr_all.shape)\n",
        "    X_te_last = scaler.transform(X_te_last.reshape(-1,X_te_last.shape[-1])).reshape(X_te_last.shape)\n",
        "\n",
        "    tr_idx, va_idx = split_by_units(u_tr_all, cfg.val_ratio_units, cfg.seed)\n",
        "    X_tr, y_tr, u_tr, c_tr = X_tr_all[tr_idx], y_tr_all[tr_idx], u_tr_all[tr_idx], c_tr_all[tr_idx]\n",
        "    X_va, y_va, u_va, c_va = X_tr_all[va_idx], y_tr_all[va_idx], u_tr_all[va_idx], c_tr_all[va_idx]\n",
        "\n",
        "    y_te_raw = y_te_last.copy()\n",
        "\n",
        "    if cfg.normalize_y:\n",
        "        y_tr = y_tr/cfg.max_rul\n",
        "        y_va = y_va/cfg.max_rul\n",
        "        y_te = y_te_last/cfg.max_rul\n",
        "    else:\n",
        "        y_te = y_te_last\n",
        "\n",
        "    return {\n",
        "        \"scaler\": scaler, \"kept_cols\": kept_cols,\n",
        "        \"train\": (X_tr,y_tr,u_tr,c_tr),\n",
        "        \"val\": (X_va,y_va,u_va,c_va),\n",
        "        \"test_last\": (X_te_last,y_te,u_te_last,c_te_last),\n",
        "        \"test_last_y_raw\": y_te_raw\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Variant specs\n",
        "# ---------------------------\n",
        "VARIANTS = [\n",
        "    {\"name\":\"Single Expert\",                       \"kind\":\"single\", \"routing\":\"na\",    \"physics\":False, \"full_uq\":False},\n",
        "    {\"name\":\"Dense MoE (no top-k)\",               \"kind\":\"moe\",    \"routing\":\"dense\", \"physics\":False, \"full_uq\":False},\n",
        "    {\"name\":\"MoE + Top-K\",                        \"kind\":\"moe\",    \"routing\":\"topk\",  \"physics\":False, \"full_uq\":False},\n",
        "    {\"name\":\"MoE + Top-K + Physics\",              \"kind\":\"moe\",    \"routing\":\"topk\",  \"physics\":True,  \"full_uq\":False},\n",
        "    {\"name\":\"Full (Top-K + Physics + UQ + Calib)\",\"kind\":\"moe\",    \"routing\":\"topk\",  \"physics\":True,  \"full_uq\":True},\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Training one model on one FD for one variant\n",
        "# ---------------------------\n",
        "def train_variant_on_domain(domain, variant):\n",
        "    X_tr,y_tr,u_tr,c_tr = domain[\"train\"]\n",
        "    X_va,y_va,u_va,c_va = domain[\"val\"]\n",
        "\n",
        "    if variant[\"kind\"] == \"single\":\n",
        "        model = SingleExpertBaseline(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "    else:\n",
        "        model = InterpretableMoE(n_features=X_tr.shape[-1]).to(DEVICE)\n",
        "\n",
        "    ds_tr = RULWindowDataset(X_tr,y_tr,u_tr,c_tr)\n",
        "    ds_va = RULWindowDataset(X_va,y_va,u_va,c_va)\n",
        "\n",
        "    sampler = ContiguousEngineBatchSampler(ds_tr.unit_ids, ds_tr.cycles, cfg.batch_size, cfg.block_len, shuffle=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler)\n",
        "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
        "\n",
        "    best_val=float(\"inf\"); best_state=None; bad=0\n",
        "\n",
        "    for ep in range(1, cfg.epochs+1):\n",
        "        s = ramp(ep, cfg.warmup_epochs, cfg.ramp_epochs)\n",
        "\n",
        "        # routing control\n",
        "        if variant[\"routing\"] == \"dense\" or variant[\"kind\"] == \"single\":\n",
        "            use_topk = False\n",
        "        else:\n",
        "            use_topk = (ep > cfg.warmup_epochs)\n",
        "\n",
        "        # only meaningful for topk MoE\n",
        "        noise_std = lerp(cfg.gate_noise_max, cfg.gate_noise_min, s) if (variant[\"routing\"]==\"topk\" and variant[\"kind\"]==\"moe\") else 0.0\n",
        "        temperature = lerp(cfg.temp_max, cfg.temp_min, s) if (variant[\"routing\"]==\"topk\" and variant[\"kind\"]==\"moe\") else 1.0\n",
        "\n",
        "        # weights\n",
        "        mono_w   = (cfg.lambda_mono*s) if variant[\"physics\"] else 0.0\n",
        "        smooth_w = (cfg.lambda_smooth*s) if variant[\"physics\"] else 0.0\n",
        "\n",
        "        lb_w   = (cfg.lambda_lb*s)   if variant[\"kind\"]==\"moe\" else 0.0\n",
        "        ent_w  = (cfg.lambda_ent*s)  if variant[\"kind\"]==\"moe\" else 0.0\n",
        "        div_w  = (cfg.lambda_div*s)  if variant[\"kind\"]==\"moe\" else 0.0\n",
        "        dead_w = (cfg.lambda_dead*s) if variant[\"kind\"]==\"moe\" else 0.0\n",
        "\n",
        "        model.train()\n",
        "        for xb,yb,ub,cb in dl_tr:\n",
        "            xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            mean, logv, w, lb, ent, div, dead = model(\n",
        "                xb, use_topk=use_topk, noise_std=noise_std, temperature=temperature, train=True\n",
        "            )\n",
        "\n",
        "            nll = gaussian_nll(yb, mean, logv).mean()\n",
        "            mse = F.mse_loss(mean, yb)\n",
        "            hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "\n",
        "            mono, smooth = physics_losses(mean, ub, cb) if variant[\"physics\"] else (mean.new_tensor(0.0), mean.new_tensor(0.0))\n",
        "\n",
        "            if ep <= cfg.warmup_epochs:\n",
        "                loss = 1.0*mse + 0.10*nll + cfg.huber_weight*hub\n",
        "            else:\n",
        "                loss = nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub\n",
        "\n",
        "            # add-ons\n",
        "            loss = loss + mono_w*mono + smooth_w*smooth\n",
        "            loss = loss + lb_w*lb - ent_w*ent + div_w*div + dead_w*dead\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # ---- validation objective for early stopping\n",
        "        model.eval()\n",
        "        va_loss=0.0\n",
        "        with torch.no_grad():\n",
        "            for xb,yb,ub,cb in dl_va:\n",
        "                xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
        "                mean, logv, *_ = model(xb, use_topk=use_topk, noise_std=0.0, temperature=1.0, train=False)\n",
        "                nll = gaussian_nll(yb, mean, logv).mean()\n",
        "                mse = F.mse_loss(mean, yb)\n",
        "                hub = F.huber_loss(mean, yb, delta=cfg.huber_delta)\n",
        "                va_loss += float((nll + cfg.aux_mse_weight*mse + cfg.huber_weight*hub).cpu())\n",
        "        va_loss /= max(1,len(dl_va))\n",
        "\n",
        "        if va_loss + 1e-6 < best_val:\n",
        "            best_val=va_loss\n",
        "            best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            bad=0\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=cfg.early_stop_patience:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation on LAST window per unit (RMSE)\n",
        "# ---------------------------\n",
        "def eval_rmse_last(model, domain, variant):\n",
        "    X_te, y_te, *_ = domain[\"test_last\"]\n",
        "    y_te_raw = domain[\"test_last_y_raw\"]\n",
        "\n",
        "    # point prediction choice:\n",
        "    # - for Full: use MC-dropout mean (matches UQ methodology)\n",
        "    # - otherwise: deterministic mean (faster + fair)\n",
        "    if variant[\"full_uq\"]:\n",
        "        # for Full, use_topk=True if moe-topk; else False\n",
        "        use_topk = (variant[\"kind\"]==\"moe\" and variant[\"routing\"]==\"topk\")\n",
        "        mean, var = predict_mc_mean_var(model, X_te, n_mc=cfg.mc_samples, use_topk=use_topk)\n",
        "        mean_raw = mean*cfg.max_rul if cfg.normalize_y else mean\n",
        "    else:\n",
        "        model.eval()\n",
        "        xb=torch.tensor(X_te,dtype=torch.float32).to(DEVICE)\n",
        "        use_topk = (variant[\"kind\"]==\"moe\" and variant[\"routing\"]==\"topk\")\n",
        "        with torch.no_grad():\n",
        "            mean, logv, *_ = model(xb, use_topk=use_topk, noise_std=0.0, temperature=1.0, train=False)\n",
        "        mean = mean.detach().cpu().numpy()\n",
        "        mean_raw = mean*cfg.max_rul if cfg.normalize_y else mean\n",
        "\n",
        "    rmse = math.sqrt(mean_squared_error(y_te_raw, mean_raw))\n",
        "    return rmse\n",
        "\n",
        "def eval_uq_full(model, domain, variant):\n",
        "    if not variant[\"full_uq\"]:\n",
        "        return None\n",
        "\n",
        "    X_te, _, *_ = domain[\"test_last\"]\n",
        "    y_te_raw = domain[\"test_last_y_raw\"]\n",
        "    X_va, y_va, *_ = domain[\"val\"]\n",
        "    y_va_raw = (y_va*cfg.max_rul) if cfg.normalize_y else y_va\n",
        "\n",
        "    use_topk = (variant[\"kind\"]==\"moe\" and variant[\"routing\"]==\"topk\")\n",
        "\n",
        "    mean, var = predict_mc_mean_var(model, X_te, n_mc=cfg.mc_samples, use_topk=use_topk)\n",
        "    if cfg.normalize_y:\n",
        "        mean_raw = mean*cfg.max_rul\n",
        "        var_raw  = var*(cfg.max_rul**2)\n",
        "    else:\n",
        "        mean_raw = mean; var_raw = var\n",
        "\n",
        "    scale = calibrate_scale_min_width(model, X_va, y_va_raw, cfg.pi_alpha, cfg.mc_samples, use_topk=use_topk)\n",
        "    var_raw = (scale**2)*var_raw\n",
        "\n",
        "    lo, hi = prediction_interval(mean_raw, var_raw, cfg.pi_alpha)\n",
        "    cov = coverage(y_te_raw, lo, hi)\n",
        "    width = float((hi-lo).mean())\n",
        "    return {\"coverage\": float(cov), \"width\": width, \"scale\": float(scale)}\n",
        "\n",
        "# ---------------------------\n",
        "# RUN: prepare domains + train all variants + build table\n",
        "# ---------------------------\n",
        "# auto fallback if running outside Colab\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    DATA_DIR = \"/mnt/data\"\n",
        "\n",
        "# file checks\n",
        "missing=[]\n",
        "for fd in FDS:\n",
        "    for f in [f\"train_{fd}.txt\", f\"test_{fd}.txt\", f\"RUL_{fd}.txt\"]:\n",
        "        if not os.path.exists(os.path.join(DATA_DIR, f)):\n",
        "            missing.append(os.path.join(DATA_DIR,f))\n",
        "if missing:\n",
        "    raise FileNotFoundError(\"Missing required C-MAPSS files:\\n\" + \"\\n\".join(missing))\n",
        "\n",
        "results = {v[\"name\"]: {} for v in VARIANTS}\n",
        "uq_results = {v[\"name\"]: {} for v in VARIANTS}\n",
        "\n",
        "for fd in FDS:\n",
        "    # prepare once per fd (shared scaler/kept_cols across variants for fairness)\n",
        "    domain = prepare_domain(fd, scaler=None, kept_cols=None)\n",
        "\n",
        "    for v in VARIANTS:\n",
        "        for mi in range(ENSEMBLE_SIZE):\n",
        "            set_seed(cfg.seed + mi)\n",
        "\n",
        "            model = train_variant_on_domain(domain, v)\n",
        "            rmse = eval_rmse_last(model, domain, v)\n",
        "\n",
        "            # store best if ensemble>1 (here 1)\n",
        "            key = f\"m{mi}\"\n",
        "            # keep single value\n",
        "            results[v[\"name\"]][fd] = rmse\n",
        "\n",
        "            if v[\"full_uq\"]:\n",
        "                uq = eval_uq_full(model, domain, v)\n",
        "                uq_results[v[\"name\"]][fd] = uq\n",
        "\n",
        "# build table dataframe\n",
        "rows=[]\n",
        "for v in VARIANTS:\n",
        "    name=v[\"name\"]\n",
        "    rmses=[results[name][fd] for fd in FDS]\n",
        "    rows.append({\n",
        "        \"Model\": name,\n",
        "        \"FD001 RMSE\": rmses[0],\n",
        "        \"FD002 RMSE\": rmses[1],\n",
        "        \"FD003 RMSE\": rmses[2],\n",
        "        \"FD004 RMSE\": rmses[3],\n",
        "        \"Avg RMSE\": float(np.mean(rmses)),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# pretty print markdown table\n",
        "def fmt(x): return f\"{x:.3f}\"\n",
        "print(\"\\n==============================\")\n",
        "print(\" Ablation RMSE Table (LAST window per unit)\")\n",
        "print(\"==============================\")\n",
        "print(\"| Model | FD001 RMSE | FD002 RMSE | FD003 RMSE | FD004 RMSE | Avg RMSE |\")\n",
        "print(\"|---|---:|---:|---:|---:|---:|\")\n",
        "for _,r in df.iterrows():\n",
        "    print(f\"| {r['Model']} | {fmt(r['FD001 RMSE'])} | {fmt(r['FD002 RMSE'])} | {fmt(r['FD003 RMSE'])} | {fmt(r['FD004 RMSE'])} | {fmt(r['Avg RMSE'])} |\")\n",
        "\n",
        "# optional UQ report for Full\n",
        "if PRINT_UQ_FOR_FULL:\n",
        "    for v in VARIANTS:\n",
        "        if not v[\"full_uq\"]:\n",
        "            continue\n",
        "        print(\"\\n==============================\")\n",
        "        print(f\" UQ report: {v['name']}\")\n",
        "        print(\"==============================\")\n",
        "        for fd in FDS:\n",
        "            uq = uq_results[v[\"name\"]].get(fd, None)\n",
        "            if uq is None:\n",
        "                continue\n",
        "            print(f\"{fd}: PI{int((1-cfg.pi_alpha)*100)}% coverage={uq['coverage']:.3f} | width={uq['width']:.3f} | cal_scale={uq['scale']:.3f}\")\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vAPO8bwMktmX",
        "outputId": "b1a4f53d-6a00-4653-ecf0-b1905d1475ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26452141.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_variant_on_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m             \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_rmse_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26452141.py\u001b[0m in \u001b[0;36mtrain_variant_on_domain\u001b[0;34m(domain, variant)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl_va\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_topk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_topk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m                 \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian_nll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26452141.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, use_topk, noise_std, temperature, train)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_topk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26452141.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m             result = _VF.gru(\n\u001b[0m\u001b[1;32m   1395\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVD-G6z1ktjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7SFP9T0ktge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXYmAZlgktdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}